{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678c56c1-8621-4ba5-83d4-b101b6dd7853",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9800564f-4872-4289-9d82-324b5c1144c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Robert Guthrie\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d6dc48-e8ab-4931-a24b-01c81292dfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb705119c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ae1797-218c-4142-b95c-36ee0f36fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1050, 0.6543, 0.1713]]], grad_fn=<StackBackward0>)\n",
      "(tensor([[[0.1050, 0.6543, 0.1713]]], grad_fn=<StackBackward0>), tensor([[[0.2801, 1.1698, 0.2483]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[ 0.2358,  0.6308, -0.1244]]], grad_fn=<StackBackward0>)\n",
      "(tensor([[[ 0.2358,  0.6308, -0.1244]]], grad_fn=<StackBackward0>), tensor([[[ 0.6330,  0.9679, -0.2342]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[ 0.0632,  0.0542, -0.4082]]], grad_fn=<StackBackward0>)\n",
      "(tensor([[[ 0.0632,  0.0542, -0.4082]]], grad_fn=<StackBackward0>), tensor([[[ 0.2578,  0.0794, -0.5721]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[0.1992, 0.1467, 0.0903]]], grad_fn=<StackBackward0>)\n",
      "(tensor([[[0.1992, 0.1467, 0.0903]]], grad_fn=<StackBackward0>), tensor([[[0.4422, 0.1805, 0.1576]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[0.1570, 0.2488, 0.2887]]], grad_fn=<StackBackward0>)\n",
      "(tensor([[[0.1570, 0.2488, 0.2887]]], grad_fn=<StackBackward0>), tensor([[[0.3754, 0.2905, 0.5443]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(out)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ca30cbd-b56f-4651-8777-83ca476a9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, 3) for _ in range(5)] \n",
    "inputs = torch.cat(inputs).view(5,1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f45a1f8a-fb8d-4d73-a59d-a99c4ed9ae42",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'batch_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# alternatively, we can do the entire sequence all at once.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# the first value returned by LSTM is all of the hidden states throughout\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the sequence. the second is just the most recent hidden state\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#inputs = torch.cat(inputs).view(5,1,-1)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m hidden \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# clean out hidden state\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m out, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'batch_first'"
     ]
    }
   ],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "#inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "#inputs = torch.cat(inputs).view(5,1,-1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden,batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c60d45c5-3f73-4e71-86ef-48101d043172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dca48a-2212-4402-b9a6-5e918e7da56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dc8eb-d3c9-4eba-a013-41a9b6081c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
