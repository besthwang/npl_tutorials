{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff2a5c2-d0d0-4c54-a0ff-afd71ebb1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a7dc6d-37db-4ef6-b914-b4dbae10c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall torchtext -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b68dfe-4424-4a08-b506-d52fea9dea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext==0.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989d7384-cf6c-4f8a-8fb1-efe3132a2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import *\n",
    "import nltk\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9423e9b-3f75-49e0-9e77-8898436a2fd9",
   "metadata": {},
   "source": [
    "![example of positional encoding](./images/img_positional_encoding.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2b5b11-0ea6-4d5f-b9b9-57acd07f7af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/developer/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c645d4d-b494-4690-98c4-efee904efcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I wonder what will come next\",\n",
    "             \"This is a basic example paragraph\",\n",
    "             \"Hello, what is a basic split?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7011d48f-bf37-4676-a56c-5eb9137e1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sequences = []\n",
    "# êµ¬ë‘ì  ì •ì˜\n",
    "punctuations = [',', '?', '!', '.']\n",
    "for seq in sequences:\n",
    "    tokens = word_tokenize(seq)\n",
    "    filtered_tokens = [token for token in tokens if token not in punctuations]\n",
    "    tokenized_sequences.append(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8ad5e0-0a91-468f-86bb-c39d39d62b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'wonder', 'what', 'will', 'come', 'next'],\n",
       " ['This', 'is', 'a', 'basic', 'example', 'paragraph'],\n",
       " ['Hello', 'what', 'is', 'a', 'basic', 'split']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b20d1542-0296-44b8-9b6f-bf79d5a05aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: a, Index: 0\n",
      "Token: This, Index: 6\n",
      "Token: wonder, Index: 13\n",
      "Token: next, Index: 9\n",
      "Token: basic, Index: 1\n",
      "Token: is, Index: 2\n",
      "Token: what, Index: 3\n",
      "Token: Hello, Index: 4\n",
      "Token: paragraph, Index: 10\n",
      "Token: I, Index: 5\n",
      "Token: example, Index: 8\n",
      "Token: split, Index: 11\n",
      "Token: come, Index: 7\n",
      "Token: will, Index: 12\n"
     ]
    }
   ],
   "source": [
    "# set the output to 2 decimal places without scientific notation\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "vocab = build_vocab_from_iterator(tokenized_sequences)\n",
    "stoi = vocab.get_stoi()\n",
    "for token, index in stoi.items():\n",
    "    print(f\"Token: {token}, Index: {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0896f5-008a-40f0-a871-89be34be2d26",
   "metadata": {},
   "source": [
    "vocab.get_stoi()ëŠ” {\"ë‹¨ì–´\": ì¸ë±ìŠ¤} í˜•íƒœì˜ dict ì„. \n",
    "\n",
    "``` python\n",
    "stoi = vocab.get_stoi()\n",
    "for token, index in stoi.items():\n",
    "    print(f\"Token: {token}, Index: {index}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda7c6ce-54f9-4fec-9c32-23d08c2ef5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = vocab.get_stoi()\n",
    "# index the sequences \n",
    "indexed_sequences = [[stoi[word] for word in seq] for seq in tokenized_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d958f1-9bd0-47bd-876f-402ab5a2fd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 13, 3, 12, 7, 9], [6, 2, 0, 1, 8, 10], [4, 3, 2, 0, 1, 11]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94a0466-4cdb-42d0-8ee0-6c0b69a6a22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "172a33a1-83cb-4737-b458-a8f7a263893d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[     0.53,     -0.97,     -2.19,      0.71],\n",
       "         [     0.52,      0.85,     -0.07,     -0.54],\n",
       "         [    -1.08,      0.13,     -0.13,      0.97],\n",
       "         [     0.40,      0.64,      0.17,     -0.86],\n",
       "         [    -0.99,      0.66,      0.55,     -1.31],\n",
       "         [     0.60,      0.55,     -0.57,      0.44]],\n",
       "\n",
       "        [[    -0.37,      0.42,     -0.30,     -0.33],\n",
       "         [    -0.07,     -0.86,      0.32,     -0.19],\n",
       "         [    -1.62,     -1.25,      1.27,      0.24],\n",
       "         [     1.33,      1.08,      0.00,     -0.50],\n",
       "         [    -1.11,     -1.39,      1.09,     -0.17],\n",
       "         [     0.35,      0.57,      1.54,      0.88]],\n",
       "\n",
       "        [[     0.28,     -1.27,      0.10,      0.73],\n",
       "         [    -1.08,      0.13,     -0.13,      0.97],\n",
       "         [    -0.07,     -0.86,      0.32,     -0.19],\n",
       "         [    -1.62,     -1.25,      1.27,      0.24],\n",
       "         [     1.33,      1.08,      0.00,     -0.50],\n",
       "         [    -0.72,      1.23,     -1.48,     -0.09]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the sequences to a tensor\n",
    "tensor_sequences = torch.tensor(indexed_sequences).long()\n",
    "\n",
    "# vocab size\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# embedding dimensions\n",
    "d_model = 4\n",
    "\n",
    "# create the embeddings\n",
    "lut = nn.Embedding(vocab_size, d_model) # look-up table (lut)\n",
    "\n",
    "# embed the sequence\n",
    "embeddings = lut(tensor_sequences)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b322ba4b-6688-4e20-91ac-3a457b4ca988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e97439b-45ee-4e55-b445-a01782b9a86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e64eb4-0ff1-4b61-9382-a04830ce081f",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ ë‹¨ê³„ëŠ” ê° ì‹œí€€ìŠ¤ ë‚´ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ ìœ„ì¹˜ ì¸ì½”ë”©(positional encoding)ì„ í†µí•´ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  \n",
    "ì•„ë˜ í•¨ìˆ˜ëŠ” ìœ„ ì •ì˜ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ì–¸ê¸‰í•  ë§Œí•œ ìœ ì¼í•œ ë³€í™”ëŠ” **ğ¿** ì´ **_max_length_** ë¡œ í‘œê¸°ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤.  \n",
    "ì´ëŠ” ê±°ì˜ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ì ì ˆíˆ ì¸ì½”ë”©í•  ìˆ˜ ìˆë„ë¡ ë³´ì¥í•˜ê¸° ìœ„í•´ ë³´í†µ ìˆ˜ì²œ ë‹¨ìœ„ì˜ ë§¤ìš° í° ê°’ìœ¼ë¡œ  \n",
    "ì„¤ì •ë©ë‹ˆë‹¤.   \n",
    "ì´ë¥¼ í†µí•´ ë™ì¼í•œ ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ì„ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ì— ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì¶”ê°€ ì „ì— ì ì ˆí•œ ê¸¸ì´ë¡œ  \n",
    "ì˜ë¼ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976c576-62da-45fd-a4c0-5a122f75f612",
   "metadata": {},
   "source": [
    "![positional formula](./images/pe_formula.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca2444c6-9ac8-4d83-82c6-7a6f01bdf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pe(max_length, d_model, n):\n",
    "\n",
    "  # generate an empty matrix for the positional encodings (pe)\n",
    "  pe = np.zeros(max_length*d_model).reshape(max_length, d_model) \n",
    "\n",
    "  # for each position\n",
    "  for k in np.arange(max_length):\n",
    "\n",
    "    # for each dimension\n",
    "    for i in np.arange(d_model//2):\n",
    "\n",
    "      # calculate the internal value for sin and cos\n",
    "      theta = k / (n ** ((2*i)/d_model))       \n",
    "\n",
    "      # even dims: sin   \n",
    "      pe[k, 2*i] = math.sin(theta) \n",
    "\n",
    "      # odd dims: cos               \n",
    "      pe[k, 2*i+1] = math.cos(theta)\n",
    "\n",
    "  return pe\n",
    "\n",
    "# maximum sequence length\n",
    "max_length = 10\n",
    "n = 1000\n",
    "encodings = gen_pe(max_length, d_model, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d66a9e-cf8f-4d6e-87ed-173fa8e728cf",
   "metadata": {},
   "source": [
    "The output of the encoding contains 10 position encoding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c93517a-1869-492f-b05b-3f3c4cd47ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.03161751,  0.99950004],\n",
       "       [ 0.90929743, -0.41614684,  0.0632034 ,  0.99800067],\n",
       "       [ 0.14112001, -0.9899925 ,  0.09472609,  0.99550337],\n",
       "       [-0.7568025 , -0.65364362,  0.12615407,  0.99201066],\n",
       "       [-0.95892427,  0.28366219,  0.1574559 ,  0.98752602],\n",
       "       [-0.2794155 ,  0.96017029,  0.18860029,  0.98205394],\n",
       "       [ 0.6569866 ,  0.75390225,  0.21955609,  0.97559988],\n",
       "       [ 0.98935825, -0.14550003,  0.25029236,  0.9681703 ],\n",
       "       [ 0.41211849, -0.91113026,  0.28077835,  0.95977264]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe6201-b391-4ed3-b864-1208b82159f3",
   "metadata": {},
   "source": [
    "ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´, **_max_length_** ëŠ” 10ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.  \n",
    "ì´ëŠ” í•„ìš”í•œ ê¸¸ì´ë³´ë‹¤ í¬ì§€ë§Œ, ê¸¸ì´ê°€ 7, 8, 9 ë˜ëŠ” 10ì¸ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ê°€  \n",
    "ìˆì„ ê²½ìš° ë™ì¼í•œ ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³´ì¥í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.   \n",
    "ë‹¨ì§€ ì ì ˆí•œ ê¸¸ì´ë¡œ ì˜ë¼ë‚´ë©´ ë©ë‹ˆë‹¤.  \n",
    "ì•„ë˜ì—ì„œëŠ” ì„ë² ë”©ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 6ì´ë¯€ë¡œ ì¸ì½”ë”©ì„ ì´ì— ë§ê²Œ ì˜ë¼ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cdd2440-04a3-4735-a105-1661854e6337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.03161751,  0.99950004],\n",
       "       [ 0.90929743, -0.41614684,  0.0632034 ,  0.99800067],\n",
       "       [ 0.14112001, -0.9899925 ,  0.09472609,  0.99550337],\n",
       "       [-0.7568025 , -0.65364362,  0.12615407,  0.99201066],\n",
       "       [-0.95892427,  0.28366219,  0.1574559 ,  0.98752602]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the first six tokens\n",
    "seq_length = embeddings.shape[1]\n",
    "encodings[:seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e395c10-b314-4bd1-827a-054bbe7e48e4",
   "metadata": {},
   "source": [
    "ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ê°€ ë™ì¼í•˜ë¯€ë¡œ í•˜ë‚˜ì˜ ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ë§Œ í•„ìš”í•˜ë©°,  \n",
    "ì´ë¥¼ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ ì‹œí€€ìŠ¤ì— ëª¨ë‘ ë¸Œë¡œë“œìºìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
    "ì´ ì˜ˆì œì—ì„œ ì„ë² ë”©ëœ ë°°ì¹˜ì˜ í˜•íƒœëŠ” (3,6,4)ì´ê³ , ìœ„ì¹˜ ì¸ì½”ë”©ì€ ì˜ë¼ë‚´ê¸°  \n",
    "ì „ì—ëŠ” (10,4) í˜•íƒœë¥¼ ê°€ì§€ë©°, ì˜ë¼ë‚¸ í›„ì—ëŠ” (6,4) í˜•íƒœê°€ ë©ë‹ˆë‹¤.  \n",
    "ì´ í–‰ë ¬ì€ ì´í›„ (3,6,4) ì¸ì½”ë”© í–‰ë ¬ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ë¸Œë¡œë“œìºìŠ¤íŠ¸ë©ë‹ˆë‹¤  \n",
    "(ì´ë¯¸ì§€ì—ì„œ í™•ì¸ ê°€ëŠ¥).   \n",
    "\n",
    "ë¸Œë¡œë“œìºìŠ¤íŠ¸ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ A Simple Introduction to Broadcastingì„ ì°¸ì¡°í•˜ì„¸ìš”.  \n",
    "ì´ ê³¼ì • ë•ë¶„ì— ë‘ í–‰ë ¬ì„ ë¬¸ì œì—†ì´ ë”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "\n",
    "When the positional encodings are added to the embeddings,  \n",
    "the output is the same as the image at the beginning of the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aac07a0-0f5e-48f3-85e3-43eeaf929fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.90,  0.29,  1.73,  0.89],\n",
       "         [ 1.65, -0.96,  1.77,  1.36],\n",
       "         [ 0.92, -0.20, -1.15,  1.43],\n",
       "         [ 0.11, -0.73, -0.24,  0.53],\n",
       "         [-1.85,  0.07,  1.75,  0.91],\n",
       "         [-1.98, -1.33, -1.04,  0.40]],\n",
       "\n",
       "        [[ 0.13,  0.79,  0.03,  1.79],\n",
       "         [-0.03,  0.41,  0.03,  2.61],\n",
       "         [ 1.54, -0.98,  0.38,  1.74],\n",
       "         [ 0.77, -1.34, -0.80,  0.90],\n",
       "         [-0.59, -1.53, -0.90,  1.71],\n",
       "         [-0.79, -0.43, -0.77,  0.39]],\n",
       "\n",
       "        [[ 0.37,  1.16, -1.41,  2.23],\n",
       "         [ 0.85,  0.76, -1.18,  1.43],\n",
       "         [ 0.03, -0.55,  0.07,  2.61],\n",
       "         [ 0.77, -1.55,  0.41,  1.74],\n",
       "         [-0.13, -1.01, -0.77,  0.90],\n",
       "         [-0.85,  0.06, -0.18,  0.27]]], dtype=torch.float64,\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings + torch.tensor(encodings[:seq_length]) # encodings[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a948362-6aa5-403c-ba50-3bd892a2a88c",
   "metadata": {},
   "source": [
    "ì´ ì¶œë ¥ì€ ëª¨ë¸ì˜ ë‹¤ìŒ ê³„ì¸µì¸ **ë©€í‹°í—¤ë“œ ì–´í…ì…˜(Multi-head Attention)**ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.   \n",
    "ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ ë‹¤ìŒ ê¸°ì‚¬ì—ì„œ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.  \n",
    "\n",
    "í•˜ì§€ë§Œ ì´ ê¸°ë³¸ êµ¬í˜„ì€ ì¤‘ì²© ë£¨í”„(nested loop)ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, íŠ¹íˆ $d_{model}$ ê³¼   \n",
    "**_max_length_** ê°’ì´ í´ ê²½ìš° ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.   \n",
    "ëŒ€ì‹ , PyTorch ì¤‘ì‹¬ì˜ ë” íš¨ìœ¨ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](./images/formula_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c340ab4-52be-4df0-95c2-dd5a8e0366aa",
   "metadata": {},
   "source": [
    "PyTorchì˜ ê¸°ëŠ¥ì„ í™œìš©í•˜ê¸° ìœ„í•´, ì›ë˜ì˜ ìˆ˜ì‹,   \n",
    "íŠ¹íˆ ë¶„ëª¨ ë¶€ë¶„ì„ ë¡œê·¸ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¶„ëª¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "$\\Large \\frac{1}{n^\\frac{2i}{d_model}}$\n",
    "\n",
    "ë¶„ëª¨ë¥¼ ìˆ˜ì •í•˜ê¸° ìœ„í•´ ğ‘›ì˜ ì§€ìˆ˜ë¥¼ ìŒìˆ˜ë¡œ ë§Œë“¤ì–´ ë¶„ìë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.    \n",
    "ê·¸ëŸ° ë‹¤ìŒ, ê·œì¹™ 7ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ìˆ˜ì‹ì„ ğ‘’ì˜ ì§€ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´í›„,   \n",
    "ê·œì¹™ 3ì„ ì‚¬ìš©í•˜ì—¬ ì§€ìˆ˜ë¥¼ ë¡œê·¸(log) ë°”ê¹¥ìœ¼ë¡œ êº¼ëƒ…ë‹ˆë‹¤.   \n",
    "ì´ë¥¼ ê°„ì†Œí™”í•˜ë©´ ìµœì¢… ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$\\Large \\frac{1}{n^\\frac{2i}{d_model}} = n^{-\\frac{2i}{d_{model}}} = e^{log{(n^{-\\frac{2i}{d_{model}}})}} = e^{-\\frac{2i\\ log(n)}{d_{model}}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09fb52-4367-4dc6-b27b-bef86b09bbe3",
   "metadata": {},
   "source": [
    "ì´ëŠ” ìœ„ì¹˜ ì¸ì½”ë”©ì˜ ëª¨ë“  ë¶„ëª¨ë¥¼ í•œ ë²ˆì— ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¤‘ìš”í•©ë‹ˆë‹¤.  \n",
    "ì•„ë˜ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, 4ì°¨ì› ì„ë² ë”©ì˜ ê²½ìš° í•„ìš”í•œ ë¶„ëª¨ëŠ” ë‘ ê°œë¿ì…ë‹ˆë‹¤.  \n",
    "ì´ëŠ” ğ‘–ê°€ ì°¨ì›ì„ ë‚˜íƒ€ë‚¼ ë•Œ, ë¶„ëª¨ê°€ 2ië§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ë³€í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.   \n",
    "ì´ëŸ¬í•œ íŒ¨í„´ì€ ê° ìœ„ì¹˜ì—ì„œ ë°˜ë³µë©ë‹ˆë‹¤:  \n",
    "\n",
    "![](./images/pe_formula_3.webp)\n",
    "\n",
    "Since only the highest number that i can be set to is d_model divided by 2,   \n",
    "the terms can be calculated once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51102613-5707-481d-8b20-c167f401d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "n = 100\n",
    "\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(n) / d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac8d75-95a1-43d3-bceb-905594c3e5c3",
   "metadata": {},
   "source": [
    "ì´ ì§§ì€ ì½”ë“œ ì¡°ê°ì€ í•„ìš”í•œ ëª¨ë“  ë¶„ëª¨ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì´ ì˜ˆì œì—ì„œëŠ” d_{model}ì´ 4ë¡œ ì„¤ì •ë˜ì—ˆê³ , nì€ 100ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
    "ì¶œë ¥ ê²°ê³¼ëŠ” ë‘ ê°œì˜ ë¶„ëª¨ì…ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5a009a7-7e3a-4e7c-885a-3e428566ed25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.00, 0.10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aff532-9c8f-4450-b2ac-692ac3336e82",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì„œë¶€í„°ëŠ” PyTorchì˜ ì¸ë±ì‹± ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ \n",
    "ì „ì²´ ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ëŠ” \n",
    "ğ‘˜ ë¶€í„° Lâˆ’1ê¹Œì§€ ê° ìœ„ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2175035-7a08-420c-9e73-5f9b03595cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "\n",
    "# generate the positions into a column matrix\n",
    "k = torch.arange(0, max_length).unsqueeze(1) \n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082f96b-7b8d-45dd-87d2-fdda96c8cbcb",
   "metadata": {},
   "source": [
    "ìœ„ì¹˜ì™€ ë¶„ëª¨ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ, ì‚¬ì¸(sin) ë° ì½”ì‚¬ì¸(cos) í•¨ìˆ˜ì˜ ë‚´ë¶€ ê°’ì„  \n",
    "ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](./images/pe_formula_4.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd88fc3-2d3c-4444-8a0c-16caf804bf58",
   "metadata": {},
   "source": [
    "kì™€ div_termì„ ê³±í•˜ë©´ ëª¨ë“  ìœ„ì¹˜ì— ëŒ€í•œ ì…ë ¥ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
    "PyTorchëŠ” í–‰ë ¬ì„ ìë™ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸í•˜ì—¬ ê³±ì…ˆì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.   \n",
    "ì´ ê²½ìš°, ëŒ€ì‘í•˜ëŠ” ìš”ì†Œë¼ë¦¬ ê³±í•´ì§€ëŠ” **ì•„ë‹¤ë§ˆë¥´ ê³±(Hadamard product)** ì´ë©°,  \n",
    "í–‰ë ¬ ê³±ì…ˆì´ ì•„ë‹˜ì„ ìœ ì˜í•˜ì„¸ìš”:\n",
    "\n",
    "![](./images/pe_formula_5.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2732d36-8691-48ce-9aa0-fccf88dda7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00, 0.00],\n",
       "        [1.00, 0.10],\n",
       "        [2.00, 0.20],\n",
       "        [3.00, 0.30],\n",
       "        [4.00, 0.40],\n",
       "        [5.00, 0.50],\n",
       "        [6.00, 0.60],\n",
       "        [7.00, 0.70],\n",
       "        [8.00, 0.80],\n",
       "        [9.00, 0.90]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k*div_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c737c809-7633-4530-9668-c443d8b47111",
   "metadata": {},
   "source": [
    "ì´ ê³„ì‚°ì˜ ì¶œë ¥ ê²°ê³¼ëŠ” ìœ„ ì´ë¯¸ì§€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
    "ì´ì œ ë‚¨ì€ ì‘ì—…ì€ ì…ë ¥ê°’ì„ ì‚¬ì¸(sin)ê³¼ ì½”ì‚¬ì¸(cos) í•¨ìˆ˜ì— ë„£ê³ ,   \n",
    "ì´ë¥¼ ì ì ˆíˆ í–‰ë ¬ì— ì €ì¥í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  \n",
    "\n",
    "ì´ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ì ì ˆí•œ í¬ê¸°ì˜ ë¹ˆ í–‰ë ¬ì„ ìƒì„±í•˜ë©´ ë©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c95cfa7-eecf-4faa-ac66-584935e8024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# generate an empty tensor\n",
    "pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd4507-a0de-4d04-a403-52582c2af5a3",
   "metadata": {},
   "source": [
    "ì´ì œ ì§ìˆ˜ ì—´(ì‚¬ì¸ ê°’ì„ ë‚˜íƒ€ëƒ„)ì€ pe[:, 0::2]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì´ëŠ” PyTorchì—ê²Œ ëª¨ë“  í–‰ê³¼ ì§ìˆ˜ ì—´ì„ ì„ íƒí•˜ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.   \n",
    "ë™ì¼í•˜ê²Œ í™€ìˆ˜ ì—´(ì½”ì‚¬ì¸ ê°’ì„ ë‚˜íƒ€ëƒ„)ì€ pe[:, 1::2]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
    "ì´ëŠ” PyTorchì—ê²Œ ëª¨ë“  í–‰ê³¼ í™€ìˆ˜ ì—´ì„ ì„ íƒí•˜ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.  \n",
    "kÃ—div_termì˜ ê²°ê³¼ëŠ” í•„ìš”í•œ ëª¨ë“  ì…ë ¥ê°’ì„ ì €ì¥í•˜ê³  ìˆìœ¼ë¯€ë¡œ,   \n",
    "ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì§ìˆ˜ ë° í™€ìˆ˜ ì—´ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23e1102-46af-41b0-8fba-a596a81fb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.00,  1.00,  0.00,  1.00],\n",
      "         [ 0.84,  0.54,  0.10,  1.00],\n",
      "         [ 0.91, -0.42,  0.20,  0.98],\n",
      "         [ 0.14, -0.99,  0.30,  0.96],\n",
      "         [-0.76, -0.65,  0.39,  0.92],\n",
      "         [-0.96,  0.28,  0.48,  0.88],\n",
      "         [-0.28,  0.96,  0.56,  0.83],\n",
      "         [ 0.66,  0.75,  0.64,  0.76],\n",
      "         [ 0.99, -0.15,  0.72,  0.70],\n",
      "         [ 0.41, -0.91,  0.78,  0.62]]])\n"
     ]
    }
   ],
   "source": [
    "# set the odd values (columns 1 and 3)\n",
    "pe[:, 0::2] = torch.sin(k * div_term)\n",
    "\n",
    "# set the even values (columns 2 and 4)\n",
    "pe[:, 1::2] = torch.cos(k * div_term)\n",
    "     \n",
    "# add a dimension for broadcasting across sequences: optional       \n",
    "pe = pe.unsqueeze(0)\n",
    "\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79215dc-a422-4678-a4df-44fe003b41f3",
   "metadata": {},
   "source": [
    "ì´ ê°’ë“¤ì€ ì¤‘ì²© for-ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–»ì€ ê°’ê³¼ ë™ì¼í•©ë‹ˆë‹¤.   \n",
    "ìš”ì•½í•˜ìë©´, ì•„ë˜ëŠ” ëª¨ë“  ì½”ë“œë¥¼ í•˜ë‚˜ë¡œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eeb2adc-f61d-4cf7-b70a-2b6d1c4eb89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.00,  1.00,  0.00,  1.00],\n",
       "         [ 0.84,  0.54,  0.10,  1.00],\n",
       "         [ 0.91, -0.42,  0.20,  0.98],\n",
       "         [ 0.14, -0.99,  0.30,  0.96],\n",
       "         [-0.76, -0.65,  0.39,  0.92],\n",
       "         [-0.96,  0.28,  0.48,  0.88],\n",
       "         [-0.28,  0.96,  0.56,  0.83],\n",
       "         [ 0.66,  0.75,  0.64,  0.76],\n",
       "         [ 0.99, -0.15,  0.72,  0.70],\n",
       "         [ 0.41, -0.91,  0.78,  0.62]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 10\n",
    "d_model = 4\n",
    "n = 100\n",
    "\n",
    "def gen_pe(max_length, d_model, n):\n",
    "  # calculate the div_term\n",
    "  div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(n) / d_model))\n",
    "\n",
    "  # generate the positions into a column matrix\n",
    "  k = torch.arange(0, max_length).unsqueeze(1)\n",
    "\n",
    "  # generate an empty tensor\n",
    "  pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "  # set the even values\n",
    "  pe[:, 0::2] = torch.sin(k * div_term)\n",
    "\n",
    "  # set the odd values\n",
    "  pe[:, 1::2] = torch.cos(k * div_term)\n",
    "\n",
    "  # add a dimension       \n",
    "  pe = pe.unsqueeze(0)        \n",
    "\n",
    "  # the output has a shape of (1, max_length, d_model)\n",
    "  return pe                           \n",
    "\n",
    "gen_pe(max_length, d_model, n)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5df7e-c332-47e3-bdab-d4e16fa404d2",
   "metadata": {},
   "source": [
    "ë” ë³µì¡í•˜ê¸´ í•˜ì§€ë§Œ, PyTorchëŠ” í–¥ìƒëœ ì„±ëŠ¥ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì—   \n",
    "ì´ êµ¬í˜„ ë°©ì‹ì„ ë¨¸ì‹  ëŸ¬ë‹ì— ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7e1c5-5e9d-48cd-9f11-56fcbd10955f",
   "metadata": {},
   "source": [
    "**Positional Encoding in Transformers**\n",
    "\n",
    "ì´ì œ ë³µì¡í•œ ì‘ì—…ì´ ëë‚¬ìœ¼ë¯€ë¡œ êµ¬í˜„ì€ ë¹„êµì  ê°„ë‹¨í•©ë‹ˆë‹¤.   \n",
    "ì´ êµ¬í˜„ì€ The Annotated Transformerì™€ PyTorchì—ì„œ íŒŒìƒë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
    "ì°¸ê³ ë¡œ nì˜ ê¸°ë³¸ê°’ì€ 10,000ì´ë©°, max_lengthì˜ ê¸°ë³¸ê°’ì€ 5,000ì…ë‹ˆë‹¤.  \n",
    "\n",
    "ì´ êµ¬í˜„ì€ ë˜í•œ **ë“œë¡­ì•„ì›ƒ(dropout)** ì„ í¬í•¨í•©ë‹ˆë‹¤.   \n",
    "ë“œë¡­ì•„ì›ƒì€ ì£¼ì–´ì§„ í™•ë¥ pì— ë”°ë¼ ì…ë ¥ ìš”ì†Œ ì¼ë¶€ë¥¼ ëœë¤í•˜ê²Œ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.     \n",
    "ì´ëŠ” ì •ê·œí™”(regularization)ì— ë„ì›€ì„ ì£¼ë©°, ë‰´ëŸ°ë“¤ì´ ì„œë¡œ ê³¼ë„í•˜ê²Œ   \n",
    "ì˜ì¡´(co-adapting)í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.  \n",
    "ì¶œë ¥ê°’ì€ ë˜í•œ $\\frac{1}{1-p}$ë¡œ ìŠ¤ì¼€ì¼ë§ë©ë‹ˆë‹¤.  \n",
    "ì´ ê¸°ì‚¬ì—ì„œ ë“œë¡­ì•„ì›ƒì— ëŒ€í•´ ê¹Šì´ ë‹¤ë£¨ì§€ëŠ” ì•Šìœ¼ë‹ˆ, ìì„¸í•œ ë‚´ìš©ì€   \n",
    "**ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´(Dropout Layer)** ì— ê´€í•œ ê¸°ì‚¬ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.  \n",
    "\n",
    "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ì— ë“œë¡­ì•„ì›ƒì—   \n",
    "ìµìˆ™í•´ì§€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.   \n",
    "ë“œë¡­ì•„ì›ƒì€ ê±°ì˜ ëª¨ë“  ë‹¤ë¥¸ ê³„ì¸µì—ì„œë„ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88e3863e-8c50-4476-9565-135de343f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      d_model:      dimension of embeddings\n",
    "      dropout:      randomly zeroes-out some of the input\n",
    "      max_length:   max sequence length\n",
    "    \"\"\"\n",
    "    # inherit from Module\n",
    "    super().__init__()     \n",
    "\n",
    "    # initialize dropout                  \n",
    "    self.dropout = nn.Dropout(p=dropout)      \n",
    "\n",
    "    # create tensor of 0s\n",
    "    pe = torch.zeros(max_length, d_model)    \n",
    "\n",
    "    # create position column   \n",
    "    k = torch.arange(0, max_length).unsqueeze(1)  \n",
    "\n",
    "    # calc divisor for positional encoding \n",
    "    div_term = torch.exp(                                 \n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    # calc sine on even indices\n",
    "    pe[:, 0::2] = torch.sin(k * div_term)    \n",
    "\n",
    "    # calc cosine on odd indices   \n",
    "    pe[:, 1::2] = torch.cos(k * div_term)  \n",
    "\n",
    "    # add dimension     \n",
    "    pe = pe.unsqueeze(0)          \n",
    "\n",
    "    # buffers are saved in state_dict but not trained by the optimizer                        \n",
    "    self.register_buffer(\"pe\", pe)                        \n",
    "\n",
    "  def forward(self, x: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x:        embeddings (batch_size, seq_length, d_model)\n",
    "    \n",
    "    Returns:\n",
    "                embeddings + positional encodings (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    # add positional encoding to the embeddings\n",
    "    x = x + self.pe[:, : x.size(1)].requires_grad_(False) \n",
    "\n",
    "    # perform dropout\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6eb46-7e65-4fc2-bf18-2a9112efffcf",
   "metadata": {},
   "source": [
    "**Forward Pass**\n",
    "\n",
    "To perform the forward pass, the same embedded sequences from earlier can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a269f5ca-66c2-4110-9fb5-3884b7fde89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.90, -0.71,  1.73, -0.11],\n",
       "         [ 0.81, -1.50,  1.74,  0.36],\n",
       "         [ 0.01,  0.22, -1.21,  0.43],\n",
       "         [-0.03,  0.26, -0.34, -0.47],\n",
       "         [-1.09,  0.73,  1.63, -0.09],\n",
       "         [-1.02, -1.61, -1.20, -0.59]],\n",
       "\n",
       "        [[ 0.13, -0.21,  0.03,  0.79],\n",
       "         [-0.88, -0.13,  0.00,  1.61],\n",
       "         [ 0.63, -0.56,  0.31,  0.74],\n",
       "         [ 0.63, -0.35, -0.90, -0.10],\n",
       "         [ 0.17, -0.87, -1.02,  0.72],\n",
       "         [ 0.17, -0.72, -0.93, -0.60]],\n",
       "\n",
       "        [[ 0.37,  0.16, -1.41,  1.23],\n",
       "         [ 0.01,  0.22, -1.21,  0.43],\n",
       "         [-0.88, -0.13,  0.00,  1.61],\n",
       "         [ 0.63, -0.56,  0.31,  0.74],\n",
       "         [ 0.63, -0.35, -0.90, -0.10],\n",
       "         [ 0.11, -0.22, -0.33, -0.71]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417ba32-3dee-482f-a531-d297efd02656",
   "metadata": {},
   "source": [
    "ì‹œí€€ìŠ¤ê°€ ì„ë² ë”©ëœ í›„, ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
    "ë“œë¡­ì•„ì›ƒì€ ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì¸ì½”ë”© ê°„ì˜ í•©ì‚°ì„ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ 0.0ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
    "ê°’ì´ ì²˜ìŒë¶€í„° êµ¬í˜„í•œ ê°’ê³¼ ë‹¤ë¥¸ ì´ìœ ëŠ” nì˜ ê¸°ë³¸ê°’ì´ 100ì´ ì•„ë‹Œ 10,000ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5456644e-9832-41c4-bba8-82cd8999d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pe',\n",
       "              tensor([[[ 0.00,  1.00,  0.00,  1.00],\n",
       "                       [ 0.84,  0.54,  0.01,  1.00],\n",
       "                       [ 0.91, -0.42,  0.02,  1.00],\n",
       "                       [ 0.14, -0.99,  0.03,  1.00],\n",
       "                       [-0.76, -0.65,  0.04,  1.00],\n",
       "                       [-0.96,  0.28,  0.05,  1.00],\n",
       "                       [-0.28,  0.96,  0.06,  1.00],\n",
       "                       [ 0.66,  0.75,  0.07,  1.00],\n",
       "                       [ 0.99, -0.15,  0.08,  1.00],\n",
       "                       [ 0.41, -0.91,  0.09,  1.00]]]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "max_length = 10\n",
    "dropout = 0.0\n",
    "\n",
    "# create the positional encoding matrix\n",
    "pe = PositionalEncoding(d_model, dropout, max_length)\n",
    "\n",
    "# preview the values\n",
    "pe.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67f9b5-f320-4d5b-9b03-b7476f6abec3",
   "metadata": {},
   "source": [
    "í•©ì‚°í•˜ê¸° ì „ì— ì‹œí€€ìŠ¤ì˜ í˜•íƒœëŠ” (batch_size,seq_length,d_model), ì¦‰ (3,6,4)ì…ë‹ˆë‹¤.  \n",
    "ìœ„ì¹˜ ì¸ì½”ë”©ë„ ì˜ë¼ë‚´ê³  ë¸Œë¡œë“œìºìŠ¤íŠ¸ëœ í›„ ë™ì¼í•œ í¬ê¸°ë¥¼ ê°€ì§€ë¯€ë¡œ,   \n",
    "ìˆœë°©í–¥ ì „ë‹¬(forward pass)ì˜ ì¶œë ¥ í¬ê¸°ë„ (batch_size,seq_length,d_model),    \n",
    "ì¦‰ (3,6,4)ë¡œ ìœ ì§€ë©ë‹ˆë‹¤. ì´ëŠ” 4ì°¨ì› ê³µê°„ì— ì„ë² ë”©ëœ 6ê°œì˜ í† í°ìœ¼ë¡œ ì´ë£¨ì–´ì§„   \n",
    "3ê°œì˜ ì‹œí€€ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ë©°, ìœ„ì¹˜ ì¸ì½”ë”©ì„ í†µí•´ ê° í† í°ì˜ ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abbc2d05-18bf-4139-9946-50fcade3f6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.90,  0.29,  1.73,  0.89],\n",
       "         [ 1.65, -0.96,  1.75,  1.36],\n",
       "         [ 0.92, -0.20, -1.19,  1.43],\n",
       "         [ 0.11, -0.73, -0.31,  0.53],\n",
       "         [-1.85,  0.07,  1.67,  0.91],\n",
       "         [-1.98, -1.33, -1.15,  0.41]],\n",
       "\n",
       "        [[ 0.13,  0.79,  0.03,  1.79],\n",
       "         [-0.03,  0.41,  0.01,  2.61],\n",
       "         [ 1.54, -0.98,  0.33,  1.74],\n",
       "         [ 0.77, -1.34, -0.87,  0.90],\n",
       "         [-0.59, -1.53, -0.98,  1.72],\n",
       "         [-0.79, -0.43, -0.88,  0.40]],\n",
       "\n",
       "        [[ 0.37,  1.16, -1.41,  2.23],\n",
       "         [ 0.85,  0.76, -1.20,  1.43],\n",
       "         [ 0.03, -0.55,  0.02,  2.61],\n",
       "         [ 0.77, -1.55,  0.34,  1.74],\n",
       "         [-0.13, -1.01, -0.86,  0.90],\n",
       "         [-0.85,  0.06, -0.28,  0.28]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8fcff-839b-4b39-87d2-31efc9e895f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
