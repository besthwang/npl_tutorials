{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fb66df-f335-46d4-abb9-32959946eb98",
   "metadata": {},
   "source": [
    "# [Understanding Image Patch Embeddings](https://medium.com/correll-lab/understanding-image-patch-embeddings-3d66c14fe7ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39f638",
   "metadata": {},
   "source": [
    "- https://npclinic3.tistory.com/6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15c21e-bd8a-4a78-a0c2-e5f95237f629",
   "metadata": {},
   "source": [
    "트랜스포머 아키텍처가 강력한 이유는 텍스트, 이미지, 그 외의 어떤 데이터 및 그 조합을<br> \n",
    "구분하지 않기 때문입니다. “어텐션(Attention)” 모델은 시퀀스 내 모든 토큰 간의 자기 <br>\n",
    "유사도(self-similarity)를 계산하여, 어떤 형태의 데이터든 요약하고 생성할 수 있게 합니다.<br> \n",
    "비전 트랜스포머(Vision Transformer)는 이미지를 정사각형 패치로 분할한 뒤, 이를 평탄화(flatten)하여 <br>\n",
    "하나의 벡터 임베딩으로 만듭니다. 이렇게 되면 텍스트 임베딩(또는 다른 어떤 임베딩)과 똑같은 <br>\n",
    "방식으로 처리할 수 있으며, 심지어 다른 데이터 타입과도 연결(concatenate)할 수 있습니다. <br>\n",
    "종종 패치를 생성하는 단계는 첫 번째 학습 가능한 비선형 변환(learnable non-linear transformation)과 <br>\n",
    "결합되는데, 이를 위해 2D 합성곱(convolution)을 사용하며, 이 부분은 이해하기가 쉽지 않습니다. <br>\n",
    "본 글은 바로 이 단계에 대해 깊이 있게 탐구합니다. 또한, 여러분이 Colab 워크시트에 코드 조각을 <br>\n",
    "그대로 복사해 붙여 넣어 따라 할 수 있도록 작성되었습니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1ef48d-4b38-4db3-b54f-f01bd8057d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "img_size = (32,32) # We will resize MNIST images to this size\n",
    "batch_size = 4\n",
    "\n",
    "transform = T.Compose([\n",
    "  T.ToTensor(),\n",
    "  T.Resize(img_size)\n",
    "])\n",
    "\n",
    "train_set = MNIST(\n",
    "  root=\"./../datasets\", train=True, download=True, transform=transform\n",
    "  )\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "batch = next(iter(train_loader)) # loads the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e2725d-cf11-4195-9448-90b57494d7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# batch[0] contains the images and batch[1] the labels\n",
    "images = batch[0]\n",
    "labels = batch[1]\n",
    "\n",
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(1, batch_size, figsize=(12, 4))\n",
    "\n",
    "# Iterate through the batch of images and labels\n",
    "for i in range(batch_size):\n",
    "  # Convert the image tensor to a NumPy array and remove the channel dimension if it's a grayscale image\n",
    "  image_np = images[i].numpy().squeeze()\n",
    "\n",
    "  # Display the image in the corresponding subplot\n",
    "  axes[i].imshow(image_np, cmap='gray')  # Use 'gray' cmap for grayscale images\n",
    "  axes[i].set_title(f\"Class: {labels[i].item()}\") # Assuming labels are tensors, use .item() to get the value\n",
    "  axes[i].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151ef5e-d1d8-48f2-b541-87e96c884c33",
   "metadata": {},
   "source": [
    "## Creating Image Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386541cc",
   "metadata": {},
   "source": [
    "Here is the code that breaks the image apart using Torch’ unfold operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b56ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Image and patch sizes\n",
    "img_size = (32, 32)\n",
    "patch_size = (8, 8)\n",
    "n_channels = 1\n",
    "\n",
    "image = batch[0][1].unsqueeze(0)\n",
    "\n",
    "# Patch Class\n",
    "# class Patch(nn.Module):\n",
    "#     def __init__(self, img_size, patch_size, n_channels):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.n_channels = n_channels\n",
    "\n",
    "#     def forward(self, x): # B x C x H X W\n",
    "#         x = x.unfold(\n",
    "#             2, self.patch_size[0], self.patch_size[0]\n",
    "#             ).unfold(\n",
    "#                 3,self.patch_size[1],self.patch_size[1]\n",
    "#                 )  # (B, C, P_row, P_col, P_height, P_width)\n",
    "#         x = x.flatten(2)  #(B, C, P_row*P_col*P_height*P_width)\n",
    "#         x = x.transpose(1, 2)  # (B,  P_row*P_col*P_height*P_width, C)\n",
    "#         return x\n",
    "\n",
    "# Instantiate model\n",
    "#patch = Patch(img_size, patch_size, n_channels)\n",
    "\n",
    "# Extract patches\n",
    "# with torch.no_grad():\n",
    "#     patches = patch(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155784ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d39bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d3ee0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ccb315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x.unfold(2, patch_size[0],patch_size[0])\n",
    "x2 = x1.unfold(3, patch_size[1],patch_size[1])\n",
    "x3 = x2.flatten(2)\n",
    "x4 = x3.transpose(1,2)\n",
    "patches = x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796d0f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patches = patches.squeeze(0)  # Remove batch dimension -> (P, d_model)\n",
    "patches = patches.view(-1, patch_size[0], patch_size[1]) # reshape back into 8x8\n",
    "\n",
    "npatches = img_size[0] // patch_size[0]\n",
    "# Plot patches\n",
    "fig, axs = plt.subplots(npatches, npatches, figsize=(6, 6))  # 4x4 grid for (32x32) -> 16 patches\n",
    "\n",
    "for i in range(npatches):\n",
    "    for j in range(npatches):\n",
    "        patch_idx = i * npatches + j  # Patch index\n",
    "        axs[i, j].imshow(patches[patch_idx], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axs[i, j].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73301a1",
   "metadata": {},
   "source": [
    "## Creating Patch Embeddings\n",
    "이 접근 방식은 임베딩의 차원을 원본 이미지 차원의 배수로 제한한다는 점을 눈치챘을 수도 있습니다.<br>\n",
    "이 제한은 unfold 연산 뒤에 선형 투영(linear projection) 을 적용함으로써 바꿀 수 있으며, <br>\n",
    "이렇게 하면 학습 가능한 임베딩을 만들 수 있습니다. <br>\n",
    "\n",
    "![](../images/Patches.webp) \n",
    "\n",
    "Patches after a linear transformation with the identity matrix(left),<br>\n",
    "after a linear transformation with random weights(middle), <br>\n",
    "and a linear transformation with random weights and bias terms.  <br>\n",
    "\n",
    "또한 이러한 임베딩은 시각화를 위해 다시 2D 텐서로 변환되었으며, 이를 통해 선형 투영이 패치 단위로<br>\n",
    "어떻게 작동하는지 보여줍니다. `nn.Linear` 클래스를 `단위 행렬(identity matrix)`로 초기화하면 <br>\n",
    "원본 데이터가 그대로 보존됨을 확인할 수 있습니다. 무작위 가중치(random weights)를 사용하면, 값이 0인 <br>\n",
    "이미지 부분은 그대로 유지됨을 볼 수 있습니다. 마지막으로 bias 항을 추가하면 변환이 모든 패치에 동일하게 <br>\n",
    "영향을 미친다는 사실을 알 수 있는데, 비어 있는 패치들이 모두 똑같은 bias 값을 갖는 것으로 나타납니다. <br>\n",
    "\n",
    "이제 새롭게 정의된 클래스는 `PatchEmbedding`이라 불리며, 이를 인스턴스화하는 코드 한 줄도 함께 제시됩니다.<br>\n",
    "여기서 새 변수` d_model`을 도입하는데, 이는 출력 임베딩의 원하는 차원을 의미합니다. 이제 이 값은 어떤 숫자든 <br> \n",
    "될 수 있습니다. 위의 그림에서는 `d_model=64`를 선택했지만, 더 이상 제한은 없습니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, n_channels, d_model):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Linear projection layer to map each patch to d_model\n",
    "        self.linear_proj = nn.Linear(patch_size[0] * patch_size[1] * n_channels, d_model,bias=False)\n",
    "        # The next two lines are unnecessary, but help to visualize that the linear \n",
    "        # projection operates along the correct dimensions\n",
    "        #with torch.no_grad():\n",
    "        #  self.linear_proj.weight.copy_(torch.eye(self.linear_proj.weight.shape[0]))\n",
    "       \n",
    "    def forward(self, x): # B x C x H X W\n",
    "\n",
    "        x = x.unfold(\n",
    "            2, self.patch_size[0], self.patch_size[0]\n",
    "            ).unfold(\n",
    "                3,self.patch_size[1],self.patch_size[1]\n",
    "                )  # (B, C, P_row, P_col, P_height, P_width)\n",
    "        \n",
    "        B, C, P_row, P_col, P_height, P_width = x.shape\n",
    "        x = x.reshape(B,C,P_row*P_col,P_height*P_width)\n",
    "        x = self.linear_proj(x)  # (B*N, d_model)\n",
    "  \n",
    "        \n",
    "        x = x.flatten(2)  #(B, C, P_row*P_col*P_height*P_width)\n",
    "        x = x.transpose(1, 2)  # (B,  P_row*P_col*P_height*P_width, C)\n",
    "\n",
    "        x = x.view(B, -1, self.d_model)\n",
    "      \n",
    "        return x\n",
    "\n",
    "d_model = 64\n",
    "# Instantiate model\n",
    "patch = PatchEmbedding(img_size, patch_size, n_channels, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb1e7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_proj = nn.Linear(patch_size[0] * patch_size[1] * n_channels, d_model,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f800a70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 1, C: 1, P_row: 4, P_col: 4, P_height: 8,         P_width: 8\n"
     ]
    }
   ],
   "source": [
    "x = image.clone()\n",
    "x1 = x.unfold(2, patch_size[0],patch_size[0])\n",
    "x2 = x1.unfold(3, patch_size[1],patch_size[1])\n",
    "B, C, P_row, P_col, P_height, P_width = x2.shape\n",
    "print(f\"B: {B}, C: {C}, P_row: {P_row}, P_col: {P_col}, P_height: {P_height}, \\\n",
    "        P_width: {P_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "528ec768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x3 shape: torch.Size([1, 1, 16, 64])\n",
      "x4 shape: torch.Size([1, 1, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "x3 = x2.reshape(B,C,P_row*P_col,P_height*P_width)\n",
    "print(f\"x3 shape: {x3.shape}\")\n",
    "x4 = linear_proj(x3)  # (B*N, d_model)\n",
    "print(f\"x4 shape: {x4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1486195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x5 shape: torch.Size([1, 1, 1024])\n",
      "x6 shape: torch.Size([1, 1024, 1])\n"
     ]
    }
   ],
   "source": [
    "x5 = x4.flatten(2)\n",
    "print(f\"x5 shape: {x5.shape}\")\n",
    "x6 = x5.transpose(1,2)\n",
    "print(f\"x6 shape: {x6.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b13cf1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x7 shape: torch.Size([1, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "x7 = x6.view(B, -1, d_model)\n",
    "print(f\"x7 shape: {x7.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936571f",
   "metadata": {},
   "source": [
    "## Creating Patch Embeddings with the 2D convolution  \n",
    "unfold 연산자는 꽤 번거롭고, 심지어 짜증스럽기까지 하다는 것을 눈치챘을 수도 있습니다. 이를 더 간단하게 처리할 방법이 있는데, 바로 원하는 패치 크기에 해당하는 커널 크기와 스트라이드 길이를 사용하여 **2D 합성곱(convolution)**을 수행하는 것입니다. 이렇게 하면 합성곱은 픽셀 단위가 아니라 패치 단위로 동작하게 되며, unfold와 nn.Linear를 조합했을 때와 동일한 결과를 얻을 수 있습니다. <br>\n",
    "\n",
    "![](../images/conv2d_patches.webp)  \n",
    "\n",
    "Using 2D Convoltuion to combine creating patches and linear transformation in a single step. 16x16 patches embedded into 4, 64 and 2500 dimensions(to row), and 8x8 patches embedded into 4, 64 and 2500 dimensions(bottom row).\n",
    "\n",
    "This is the revised PatchEmbedding class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ed72d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, n_channels, d_model):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.d_model = d_model  # Flattened patch size\n",
    "\n",
    "        # Conv2d to extract patches\n",
    "        self.linear_project = nn.Conv2d(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=self.d_model,  # Each patch is flattened to d_model\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x)  # (B, d_model, P_row, P_col)\n",
    "        x = x.flatten(2)  # (B, d_model, P_row * P_col) -> (B, d_model, P)\n",
    "        x = x.transpose(1, 2)  # (B, P, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2eb457",
   "metadata": {},
   "source": [
    "위에서 만든 어떤 형태의 패치 임베딩이든 비전 트랜스포머(Vision Transformer)에 입력으로 사용할 수 있습니다. 이 과정을 **2D 합성곱(Conv2d)** 으로 구현하면 가장 범용적이고 간결한 표현 방식이 됩니다. 합성곱은 각 차원마다 전용 커널을 사용하지만, 지금까지는 모든 패치에 동일한 커널을 사용해왔다는 점을 주목하세요.\n",
    "\n",
    "이를 그림으로 보여주고, 합성곱이 엉뚱한 동작을 하지 않는다는 것을 확인하기 위해, 커널 가중치를 초기화해서 각 커널이 패치당 단일 픽셀만 추출하도록 설정할 수 있습니다. 아래의 코드는 패치 크기가 (8,8)이고, 결과 임베딩 차원 `d_model=64`인 경우에 동작합니다. 이 코드를 PatchEmbedding 클래스의 `__init__` 메서드 마지막 부분에 추가하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7086ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, n_channels, d_model):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.d_model = d_model  # Flattened patch size\n",
    "\n",
    "        # Conv2d to extract patches\n",
    "        self.linear_project = nn.Conv2d(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=self.d_model,  # Each patch is flattened to d_model\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        \"\"\"Initialize Conv2d to extract patches without transformation.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            identity_kernel = torch.zeros(\n",
    "                self.d_model, self.n_channels, *self.patch_size\n",
    "            )  # Shape: (64, 1, 8, 8)\n",
    "\n",
    "            for i in range(self.d_model):  \n",
    "                row = i // self.patch_size[1]  # Row index in the patch\n",
    "                col = i % self.patch_size[1]   # Column index in the patch\n",
    "                identity_kernel[i, 0, row, col] = 1  # Place a 1 at the correct pixel position\n",
    "\n",
    "            self.linear_project.weight.copy_(identity_kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x)  # (B, d_model, P_row, P_col)\n",
    "        x = x.flatten(2)  # (B, d_model, P_row * P_col) -> (B, d_model, P)\n",
    "        x = x.transpose(1, 2)  # (B, P, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1d359",
   "metadata": {},
   "source": [
    "As you can see the identity_kernel tensor maintains d_model entries, one for each dimension, and only one pixel of each patch is set to one, thereby extracting just that pixel. A simpler way to do this is to simple cast a d_model x d_model identity matrix into d_model matrices of patch_size:\n",
    "\n",
    "``` python\n",
    "        identity_matrix = torch.eye(self.d_model)\n",
    "        identity_kernel = identity_matrix.view(d_model, 1, *patch_size)  # Shape: (64, 1, 8, 8)\n",
    "\n",
    "        with torch.no_grad():\n",
    "             self.linear_project.weight.copy_(identity_kernel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba61d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
