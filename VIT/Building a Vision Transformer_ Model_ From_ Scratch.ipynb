{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735b9fef-1c27-49ae-8132-fa4f9feb6da0",
   "metadata": {},
   "source": [
    "# [Building a Vision Transformer Model From Scratch](https://medium.com/correll-lab/building-a-vision-transformer-model-from-scratch-a3054f707cc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df295c-b8b4-490a-9dcc-2b0f2766295c",
   "metadata": {},
   "source": [
    "## Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0083d8c-b62a-4bb5-ae83-0121924f219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47223b82-d3d0-4618-afe1-656a943dda42",
   "metadata": {},
   "source": [
    "우리는 PyTorch를 사용하여 Vision Transformer를 구축할 예정이므로,  <br>\n",
    "이 튜토리얼에서 사용할 PyTorch와 기타 라이브러리들을 임포트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f3327d-9243-4a35-8814-f3e51d90586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83857f-f61b-4571-bd1b-0efe2d3018e6",
   "metadata": {},
   "source": [
    "또한 입력 이미지를 리사이즈하고 텐서로 변환하기 위해 `torchvision.transforms`를 <br>\n",
    "임포트해야 합니다. 입력 이미지를 리사이즈하는 것은 선택 사항입니다. <br>\n",
    "다만, 이미지 크기가 패치 크기로 나누어떨어지도록만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a4c224-c5ee-43c6-83f5-dcfcdc137af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ad242-43fb-498f-a0cc-ecfeb7366911",
   "metadata": {},
   "source": [
    "우리는 옵티마이저로 Adam을 사용할 예정이므로, `torch.optim`에서 <br>\n",
    "이를 임포트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2bc0bc-e8d9-4937-8ee6-70c0bf4df9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998ced9-38f1-4e85-9e27-d7f4a585a542",
   "metadata": {},
   "source": [
    "이번 튜토리얼에서 사용할 MNIST 데이터셋을 `torchvision`에서 임포트할 예정입니다. <br>\n",
    "또한 데이터를 불러오는 데 도움이 되는 PyTorch의 `DataLoader`를 사용할 것이므로, <br>\n",
    "그것도 함께 임포트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36d79b5-f87f-49b0-8733-3a39a92ccdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f59aa-69aa-432f-839a-e0434d3a78f0",
   "metadata": {},
   "source": [
    "마지막으로, 위치 인코딩을 생성할 때 사인(sin)과 코사인(cosine) 연산을<br>\n",
    "수행하기 위해 사용할 numpy를 임포트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae98cdf0-f599-435a-a700-c1351d905329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900471d-e905-4251-9d13-771ab9ad0681",
   "metadata": {},
   "source": [
    "## Patch Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3017ca-b143-4696-8aa2-76d0e02d9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model # Dimensionality of Model\n",
    "    self.img_size = img_size # Image Size\n",
    "    self.patch_size = patch_size # Patch Size\n",
    "    self.n_channels = n_channels # Number of Channels\n",
    "\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  # B: Batch Size\n",
    "  # C: Image Channels\n",
    "  # H: Image Height\n",
    "  # W: Image Width\n",
    "  # P_col: Patch Column\n",
    "  # P_row: Patch Row\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n",
    "\n",
    "    x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n",
    "\n",
    "    x = x.transpose(1, 2) # (B, d_model, P) -> (B, P, d_model)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eec1c8-207d-499b-8a52-32c31515daad",
   "metadata": {},
   "source": [
    "비전 트랜스포머를 만드는 첫 번째 단계는 입력 이미지를 패치로 분할하고, <br>\n",
    "이 패치들의 선형 임베딩 시퀀스를 생성하는 것입니다. <br>\n",
    "이는 PyTorch의 Conv2d 메서드를 사용하여 구현할 수 있습니다. <br>\n",
    "<br>\n",
    "Conv2d 메서드는 입력 이미지를 받아 패치로 나눈 뒤, 모델의 너비와 동일한 크기의<br>\n",
    "선형 투영을 제공합니다. kernel_size와 stride를 패치 크기로 설정함으로써, <br>\n",
    "패치들이 올바른 크기를 가지며 서로 겹치지 않도록 보장할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
