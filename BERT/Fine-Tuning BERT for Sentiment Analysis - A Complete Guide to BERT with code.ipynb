{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6b58f8-5b7e-430a-a144-4d3b272ab844",
   "metadata": {},
   "source": [
    "참고문헌  \n",
    "\\[1\\] [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e1fc6-ac1d-4b34-9536-64e4e228972d",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113ff54-5f4b-40ad-afef-9920bcd35d69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1 -Load and Preprocess a Fine-Tunning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcaa102a-993e-4f94-ad49-39c72eac4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d751d86-ff5e-48a2-a90f-acccb8343d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Datasets/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5c414-4413-4e2e-a517-5d323a9b2f86",
   "metadata": {},
   "source": [
    "이전의 NLP 모델들과는 달리, BERT와 같은 트랜스포머 기반 모델은 전처리가 거의 필요하지 않다.  \n",
    "불용어(stop words)나 구두점(punctuation)을 제거하는 단계는 경우에 따라 오히려 역효과를 낳을 수 있다.  \n",
    "이러한 요소들이 입력 문장을 이해하는 데 유용한 문맥 정보를 `BERT`에게 제공하기 때문이다.  \n",
    "  \n",
    "그럼에도 불구하고, 텍스트에 포맷 문제나 원치 않는 특수 문자 등이 있는지 확인하는 작업은 여전히 중요하다.  \n",
    "전반적으로 IMDb 데이터셋은 꽤 깨끗한 편이지만, 스크래핑 과정에서 생긴 일부 흔적들이 남아 있는 것으로 보인다.  \n",
    ">예를 들어:  \n",
    "HTML 줄바꿈 태그 (\\<br />)  \n",
    "불필요한 공백(whitespace) 등  \n",
    "\n",
    "이러한 요소들은 제거해주는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702aefe3-c8ac-4de9-b1b9-34670c5458b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "A wonderful little production. <br /><br />The filming technique is very\n",
      "nAfter cleaning:\n",
      "A wonderful little production. The filming technique i  very una uming- \n"
     ]
    }
   ],
   "source": [
    "# Remove the break tags (<br />)\n",
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "# Remove unnecessary whitespace\n",
    "df['review_cleaned'] = df['review_cleaned'].replace('s+', ' ', regex=True)\n",
    "\n",
    "# Compare 72 characters of the second review before and after cleaning\n",
    "print('Before cleaning:')\n",
    "print(df.iloc[1]['review'][0:72])\n",
    "\n",
    "print('nAfter cleaning:')\n",
    "print(df.iloc[1]['review_cleaned'][0:72])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1198265-d113-4b52-aa6d-3261a340467d",
   "metadata": {},
   "source": [
    "### Encode the Sentiment:  \n",
    "전처리의 마지막 단계는 각 리뷰의 감정을 부정일 경우 0, 긍정일 경우 1로 인코딩하는 것이다.  \n",
    "이러한 라벨은 파인튜닝 과정에서 분류 헤드(classification head)를 학습시키는 데 사용된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7fc76e-c359-4d53-b71a-a9312d57414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewer  ha  mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought thi  wa  a wonderful way to  pend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Ba ically there'  a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei'  \"Love in the Time of Money\" i ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      review_cleaned  sentiment_encoded  \n",
       "0  One of the other reviewer  ha  mentioned that ...                  1  \n",
       "1  A wonderful little production. The filming tec...                  1  \n",
       "2  I thought thi  wa  a wonderful way to  pend ti...                  1  \n",
       "3  Ba ically there'  a family where a little boy ...                  0  \n",
       "4  Petter Mattei'  \"Love in the Time of Money\" i ...                  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bce89-aa0d-43c0-b68f-40d8944c17a3",
   "metadata": {},
   "source": [
    "## 3.2 – 파인튜닝 데이터 토크나이즈(Tokenize)\n",
    "전처리가 완료되면, 파인튜닝 데이터에 대해 토크나이즈(`tokenization`) 과정을 수행할 수 있다.  \n",
    "이 과정에서는 다음과 같은 작업이 이루어진다:  \n",
    "\n",
    "리뷰 텍스트를 개별 토큰(token) 으로 분할  \n",
    "[CLS]와 [SEP] 같은 특수 토큰 추가  \n",
    "\n",
    "시퀀스 길이를 맞추기 위한 패딩 처리  \n",
    "\n",
    "모델마다 요구하는 토크나이징 방식이 다르기 때문에, 해당 모델에 맞는 적절한 토크나이저를 선택하는 것이 중요하다.  \n",
    "예를 들어, GPT는 [CLS] 및 [SEP] 토큰을 사용하지 않지만, BERT는 이를 사용한다.  \n",
    "\n",
    "이번 예제에서는 `Hugging Face`의 `transformers` 라이브러리에서 제공하는 `BertTokenizer` 클래스를 사용한다.  \n",
    "이 토크나이저는 `BERT` 계열 모델과 함께 사용하도록 설계되어 있다.  \n",
    "토크나이징 작동 방식에 대한 보다 자세한 설명은 이 시리즈의 Part 1을 참고하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6eab0-a18e-4506-9929-3c8dece6da30",
   "metadata": {},
   "source": [
    "transformers 라이브러리의 토크나이저 클래스는 from_pretrained 메서드를 사용하여  \n",
    "**사전학습된 토크나이저 모델을 손쉽게 생성**할 수 있다.  \n",
    "이를 사용하는 방법은 다음과 같다:  \n",
    "\n",
    "1. 토크나이저 클래스를 import 및 인스턴스화  \n",
    "2. from_pretrained 메서드를 호출  \n",
    "3. Hugging Face 모델 저장소에 있는 토크나이저 이름(문자열 형태)을 인자로 전달\n",
    "  \n",
    "또는, 로컬에 저장된 단어 집합(vocabulary) 파일이 포함된 디렉터리 경로를 전달하는 것도 가능하다 [9].  \n",
    "\n",
    "---  \n",
    "\n",
    "이번 예제에서는 Hugging Face 모델 저장소에서 제공하는 사전학습된 토크나이저를 사용할 것이며,  \n",
    "BERT 관련으로는 다음과 같은 네 가지 주요 옵션이 있다.   \n",
    "이들은 모두 Google의 사전학습 BERT 토크나이저의 어휘를 기반으로 한다:  \n",
    "\n",
    "- bert-base-uncased  \n",
    "→ BERT Base 모델용, 대소문자 구분 안 함 (예: Cat과 cat을 동일하게 처리)\n",
    "\n",
    "- bert-base-cased  \n",
    "→ BERT Base 모델용, 대소문자 구분함 (예: Cat과 cat을 다르게 처리)\n",
    "\n",
    "- bert-large-uncased  \n",
    "→ BERT Large 모델용, 대소문자 구분 안 함\n",
    "\n",
    "- bert-large-cased  \n",
    "→ BERT Large 모델용, 대소문자 구분함\n",
    "\n",
    "`BERT Base`와 `BERT Large`는 **동일한 어휘 집합(vocabulary)** 을 사용하므로,  \n",
    "`bert-base-uncased`와 `bert-large-uncased`는 실질적으로 차이가 없고,  \n",
    "`bert-base-cased`와 `bert-large-cased`도 동일한 어휘를 사용한다.  \n",
    "\n",
    "그러나 다른 모델들에서는 동일하지 않을 수 있으므로, 확실하지 않다면 모델과 토크나이저의  \n",
    "크기를 일치시켜 사용하는 것이 가장 안전하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0b138-18a5-4ee6-836e-a22d22029bf4",
   "metadata": {},
   "source": [
    "### 대소문자 구분(cased vs uncased)을 언제 사용할까?\n",
    "(When to Use cased vs uncased:)  \n",
    "\n",
    "cased 모델과 uncased 모델 중 어떤 것을 사용할지는 데이터셋의 특성에 따라 달라진다.  \n",
    "예를 들어, IMDb 데이터셋은 인터넷 사용자들이 작성한 리뷰로 구성되어 있다.  \n",
    "이들은 대소문자 사용에 일관성이 없을 수 있다.  \n",
    "\n",
    "어떤 사용자는 문장의 첫 글자에서 대문자를 생략하기도 하고,  \n",
    "어떤 사용자는 **강조나 감정 표현(흥분, 분노 등)**을 위해 대문자를 과하게 사용하기도 한다.  \n",
    "이러한 이유로 우리는 대소문자를 무시(uncased) 하기로 결정하고,  \n",
    "bert-base-uncased 토크나이저 모델을 사용할 것이다.  \n",
    "\n",
    "하지만, 경우에 따라 대소문자를 구분하는(cased) 것이 성능 향상에 도움이 되기도 한다.  \n",
    "대표적인 예로는 개체명 인식(Named Entity Recognition, NER) 과제가 있다.  \n",
    "이 과제에서는 입력 텍스트 내에서 사람, 조직, 장소 등의 **고유 명사(Entity)** 를 식별하는 것이 목표인데,  \n",
    "이 경우 대문자의 존재 여부는 해당 단어가 사람 이름인지, 장소 이름인지 등을 판별하는 데 매우 유용하다.  \n",
    "\n",
    "따라서, 이러한 상황에서는 bert-base-cased 모델을 사용하는 것이 더 적합할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32157f09-80b9-46c1-b7f3-b26d29573319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2b61c-fb43-446d-9744-e42542cf1dcb",
   "metadata": {},
   "source": [
    "### **Encoding Process: Converting Text to Tokens to Token IDs**\n",
    "\n",
    "제 전처리된 파인튜닝 데이터를 인코딩할 차례다.  \n",
    "이 과정에서는 각 리뷰가 **토큰 ID로 이루어진 텐서(tensor)**로 변환된다.  \n",
    "\n",
    "예를 들어, 리뷰 \"I liked this movie\" 는 다음 단계를 통해 인코딩된다:  \n",
    "\n",
    "1. 소문자 변환  \n",
    "→ 우리가 bert-base-uncased를 사용하고 있으므로, 입력 문장은 모두 소문자로 변환된다.  \n",
    "\n",
    "2. 토큰 분할  \n",
    "→ BERT 어휘(bert-base-uncased의 vocabulary)에 따라 문장을 토큰 단위로 분할:  \n",
    "['i', 'liked', 'this', 'movie']  \n",
    "\n",
    "3. 특수 토큰 추가  \n",
    "→ BERT에서 기대하는 특수 토큰을 추가:  \n",
    "['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']  \n",
    "\n",
    "4. 토큰을 토큰 ID로 변환  \n",
    "→ 각 토큰을 BERT 어휘에 따라 정수형 ID로 매핑 (예: [CLS] → 101, i → 1045 등)  \n",
    "---\n",
    "이 전체 인코딩 과정은 `BertTokenizer` 클래스의 `encode` 메서드를 통해 수행할 수 있다.  \n",
    "이 메서드는 텍스트를 인코딩하여 토큰 ID 텐서를 반환하며, 다음 중 하나의 형식으로 반환 가능하다:  \n",
    "\n",
    "- PyTorch 텐서 (pt)\n",
    "- TensorFlow 텐서 (tf)\n",
    "- NumPy 배열 (np)\n",
    "\n",
    "반환 형식은 return_tensors 인자를 통해 지정할 수 있다.  \n",
    "\n",
    "> 💡 참고: Hugging Face에서는 Token ID를 종종 Input ID라고 부르기도 하며, 두 용어는 혼용되어 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe744420-0c9f-45f6-9fe5-b6735c457a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [ 101 1045 4669 2023 3185  102]\n",
      "Tokens   : ['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Encode a sample input sentence\n",
    "sample_sentence = 'I liked this movie'\n",
    "token_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\n",
    "print(f'Token IDs: {token_ids}')\n",
    "\n",
    "# Convert the token IDs back to tokens to reveal the special tokens added\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f'Tokens   : {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d97b1d-f442-4cad-a35a-0458e72f7493",
   "metadata": {},
   "source": [
    "### **Truncation and Padding:**\n",
    "\n",
    "BERT Base와 BERT Large는 입력 시퀀스가 정확히 512개 토큰일 때 처리하도록 설계되어 있다.  \n",
    "그렇다면 입력 시퀀스가 이 길이를 초과하거나 부족할 경우에는 어떻게 해야 할까?  \n",
    "정답은 바로 **잘라내기(truncation)** 와 **패딩(padding)** 이다!  \n",
    "\n",
    "**잘라내기 (Truncation):**    \n",
    "잘라내기는 지정된 길이를 초과하는 토큰들을 단순히 잘라서 제거하는 방식이다.  \n",
    "encode 메서드에서 truncation=True로 설정하고, max_length 인자를 지정하면  \n",
    "모든 인코딩된 시퀀스에 대해 길이 제한을 강제할 수 있다.  \n",
    "\n",
    "이 데이터셋의 일부 리뷰는 512개 토큰 제한을 초과하므로,  \n",
    "가장 많은 텍스트를 반영할 수 있도록 max_length=512로 설정한다.  \n",
    "만약 모든 리뷰가 512 토큰을 넘지 않는다면, max_length는 생략해도 되고,  \n",
    "기본적으로 **모델의 최대 입력 길이(512)**가 자동으로 적용된다.  \n",
    "\n",
    "또는, 학습 시간을 줄이기 위해 512보다 짧은 길이를 지정하는 것도 가능하다.  \n",
    "다만, 이 경우 모델 성능이 다소 희생될 수 있음에 유의해야 한다.  \n",
    "\n",
    "**패딩 (Padding):**    \n",
    "대부분의 리뷰는 512 토큰보다 짧기 때문에,  \n",
    "이런 경우에는 [PAD] 토큰을 추가하여 시퀀스를 512 토큰까지 확장한다.  \n",
    "이를 위해 padding='max_length' 옵션을 설정하면 된다.  \n",
    "\n",
    "자세한 내용은 Hugging Face의 encode 메서드 관련 문서를 참조하자 [10].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff22a2b-1020-4a51-9f6f-c5da3b94281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review  example.\n",
      "One of the other reviewer  ha  mentioned that after watching ju t 1 Oz epi ode you'll be hooked. They are right, a  thi  i  exactly what happened with me.The fir t thing that  truck me about Oz wa  it  brutality and unflinching  cene  of violence, which  et in right from the word GO. Tru t me, thi  i  not a  how for the faint hearted or timid. Thi   how pull  no punche  with regard  to drug ,  ex or violence. It  i  hardcore, in the cla ic u e of the word.It i  called OZ a  that i  the nickname given to the O wald Maximum Security State Penitentary. It focu e  mainly on Emerald City, an experimental  ection of the pri on where all the cell  have gla  front  and face inward ,  o privacy i  not high on the agenda. Em City i  home to many..Aryan , Mu lim , gang ta , Latino , Chri tian , Italian , Iri h and more.... o  cuffle , death  tare , dodgy dealing  and  hady agreement  are never far away.I would  ay the main appeal of the  how i  due to the fact that it goe  where other  how  wouldn't dare. Forget pretty picture  painted for main tream audience , forget charm, forget romance...OZ doe n't me  around. The fir t epi ode I ever  aw  truck me a   o na ty it wa   urreal, I couldn't  ay I wa  ready for it, but a  I watched more, I developed a ta te for Oz, and got accu tomed to the high level  of graphic violence. Not ju t violence, but inju tice (crooked guard  who'll be  old out for a nickel, inmate  who'll kill on order and get away with it, well mannered, middle cla  inmate  being turned into pri on bitche  due to their lack of  treet  kill  or pri on experience) Watching Oz, you may become comfortable with what i  uncomfortable viewing....that  if you can get in touch with your darker  ide.\n",
      "----------------------\n",
      "tensor([[  101,  2028,  1997,  1996,  2060, 12027,  5292,  3855,  2008,  2044,\n",
      "          3666, 18414,  1056,  1015, 11472,  4958,  2072, 24040,  2017,  1005,\n",
      "          2222,  2022, 13322,  1012,  2027,  2024,  2157,  1010,  1037, 16215,\n",
      "          2072,  1045,  3599,  2054,  3047,  2007,  2033,  1012,  1996, 21554,\n",
      "          1056,  2518,  2008,  4744,  2033,  2055, 11472, 11333,  2009, 24083,\n",
      "          1998,  4895, 10258,  2378,  8450,  8292,  2638,  1997,  4808,  1010,\n",
      "          2029,  3802,  1999,  2157,  2013,  1996,  2773,  2175,  1012, 19817,\n",
      "          2226,  1056,  2033,  1010, 16215,  2072,  1045,  2025,  1037,  2129,\n",
      "          2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012, 16215,  2072,\n",
      "          2129,  4139,  2053,  8595,  2063,  2007,  7634,  2000,  4319,  1010,\n",
      "          4654,  2030,  4808,  1012,  2009,  1045, 13076,  1010,  1999,  1996,\n",
      "         18856,  2050, 24582,  1057,  1041,  1997,  1996,  2773,  1012,  2009,\n",
      "          1045,  2170, 11472,  1037,  2008,  1045,  1996,  8367,  2445,  2000,\n",
      "          1996,  1051, 24547,  2094,  4555,  3036,  2110,  7279,  4221, 12380,\n",
      "          2854,  1012,  2009,  1042, 10085,  2226,  1041,  3701,  2006, 14110,\n",
      "          2103,  1010,  2019,  6388, 14925,  3508,  1997,  1996, 26927,  2006,\n",
      "          2073,  2035,  1996,  3526,  2031,  1043,  2721,  2392,  1998,  2227,\n",
      "         20546,  1010,  1051,  9394,  1045,  2025,  2152,  2006,  1996, 11376,\n",
      "          1012,  7861,  2103,  1045,  2188,  2000,  2116,  1012,  1012, 26030,\n",
      "          1010, 14163, 18525,  1010,  6080, 11937,  1010,  7402,  1010, 10381,\n",
      "          3089, 23401,  1010,  3059,  1010, 20868,  2072,  1044,  1998,  2062,\n",
      "          1012,  1012,  1012,  1012,  1051, 26450,  2571,  1010,  2331, 16985,\n",
      "          2063,  1010, 26489,  6292,  7149,  1998,  2018,  2100,  3820,  2024,\n",
      "          2196,  2521,  2185,  1012,  1045,  2052,  1037,  2100,  1996,  2364,\n",
      "          5574,  1997,  1996,  2129,  1045,  2349,  2000,  1996,  2755,  2008,\n",
      "          2009,  2175,  2063,  2073,  2060,  2129,  2876,  1005,  1056,  8108,\n",
      "          1012,  5293,  3492,  3861,  4993,  2005,  2364, 29461,  3286,  4378,\n",
      "          1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012, 11472,\n",
      "         18629,  1050,  1005,  1056,  2033,  2105,  1012,  1996, 21554,  1056,\n",
      "          4958,  2072, 24040,  1045,  2412, 22091,  4744,  2033,  1037,  1051,\n",
      "          6583,  5939,  2009, 11333, 24471, 22852,  1010,  1045,  2481,  1005,\n",
      "          1056,  1037,  2100,  1045, 11333,  3201,  2005,  2009,  1010,  2021,\n",
      "          1037,  1045,  3427,  2062,  1010,  1045,  2764,  1037, 11937,  8915,\n",
      "          2005, 11472,  1010,  1998,  2288, 16222,  2226, 21269,  2094,  2000,\n",
      "          1996,  2152,  2504,  1997,  8425,  4808,  1012,  2025, 18414,  1056,\n",
      "          4808,  1010,  2021,  1999,  9103, 14841,  3401,  1006, 15274,  3457,\n",
      "          2040,  1005,  2222,  2022,  2214,  2041,  2005,  1037, 15519,  1010,\n",
      "         24467,  2040,  1005,  2222,  3102,  2006,  2344,  1998,  2131,  2185,\n",
      "          2007,  2009,  1010,  2092,  5450,  2098,  1010,  2690, 18856,  2050,\n",
      "         24467,  2108,  2357,  2046, 26927,  2006,  7743,  2063,  2349,  2000,\n",
      "          2037,  3768,  1997,  3392,  2102,  3102,  2030, 26927,  2006,  3325,\n",
      "          1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,  2007,  2054,\n",
      "          1045,  8796, 10523,  1012,  1012,  1012,  1012,  2008,  2065,  2017,\n",
      "          2064,  2131,  1999,  3543,  2007,  2115,  9904,  8909,  2063,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    }
   ],
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "\n",
    "print(\"review  example.\");\n",
    "print(review)\n",
    "\n",
    "print(\"----------------------\")\n",
    "\n",
    "token_ids = tokenizer.encode(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143c3c2-ecbc-4483-8edc-3307ecb5db33",
   "metadata": {},
   "source": [
    "### `encode_plus`와 함께 Attention Mask 사용하기\n",
    "\n",
    "앞서 살펴본 예제에서는 데이터셋의 첫 번째 리뷰가 인코딩된 결과, `119`개의 패딩 토큰을 포함하고 있다.  \n",
    "이 상태 그대로 파인튜닝에 사용하면, BERT가 패딩 토큰에까지 주의를 기울이게 되어  \n",
    "성능 저하로 이어질 수 있다.  \n",
    "\n",
    "이를 해결하기 위해 우리는 **어텐션 마스크(attention mask)** 를 적용할 수 있다.  \n",
    "어텐션 마스크는 BERT에게 입력에서 특정 토큰(이 경우 패딩 토큰)을 무시하라고 지시하는 역할을 한다.    \n",
    "\n",
    "-----\n",
    "\n",
    "이를 구현하려면 기존의 encode 메서드 대신,  \n",
    "encode_plus 메서드를 사용하도록 코드를 수정하면 된다.  \n",
    "\n",
    "encode_plus는 Hugging Face에서 **배치 인코더(Batch Encoder)** 라고 불리는  \n",
    "딕셔너리 형태의 값을 반환하며, 다음과 같은 키를 포함한다:  \n",
    "\n",
    "- **input_ids**:  \n",
    "→ encode 메서드와 동일하게 토큰 ID들을 반환  \n",
    "\n",
    "- **token_type_ids**:  \n",
    "→ 문장 쌍 과제(Natural Language Inference, Next Sentence Prediction 등)에서\n",
    "문장 A(값 = 0)와 문장 B(값 = 1)를 구분하는 세그먼트 ID\n",
    "\n",
    "- **attention_mask**:  \n",
    "→ 0과 1로 이루어진 리스트\n",
    "→ 0은 해당 토큰이 어텐션에서 무시되어야 함을 의미 (예: 패딩 토큰)  \n",
    "→ 1은 해당 토큰이 어텐션에서 사용되어야 함을 의미  \n",
    "\n",
    "이 어텐션 마스크를 통해 BERT는 실제 의미 있는 토큰에만 집중할 수 있게 되며,  \n",
    "모델의 성능이 유지되거나 향상될 수 있다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894ef5ac-3164-4c11-861d-d9f2c32f9b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch encoder keys:\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "Attention mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "input_ids:\n",
      "tensor([[  101,  2028,  1997,  1996,  2060, 12027,  5292,  3855,  2008,  2044,\n",
      "          3666, 18414,  1056,  1015, 11472,  4958,  2072, 24040,  2017,  1005,\n",
      "          2222,  2022, 13322,  1012,  2027,  2024,  2157,  1010,  1037, 16215,\n",
      "          2072,  1045,  3599,  2054,  3047,  2007,  2033,  1012,  1996, 21554,\n",
      "          1056,  2518,  2008,  4744,  2033,  2055, 11472, 11333,  2009, 24083,\n",
      "          1998,  4895, 10258,  2378,  8450,  8292,  2638,  1997,  4808,  1010,\n",
      "          2029,  3802,  1999,  2157,  2013,  1996,  2773,  2175,  1012, 19817,\n",
      "          2226,  1056,  2033,  1010, 16215,  2072,  1045,  2025,  1037,  2129,\n",
      "          2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012, 16215,  2072,\n",
      "          2129,  4139,  2053,  8595,  2063,  2007,  7634,  2000,  4319,  1010,\n",
      "          4654,  2030,  4808,  1012,  2009,  1045, 13076,  1010,  1999,  1996,\n",
      "         18856,  2050, 24582,  1057,  1041,  1997,  1996,  2773,  1012,  2009,\n",
      "          1045,  2170, 11472,  1037,  2008,  1045,  1996,  8367,  2445,  2000,\n",
      "          1996,  1051, 24547,  2094,  4555,  3036,  2110,  7279,  4221, 12380,\n",
      "          2854,  1012,  2009,  1042, 10085,  2226,  1041,  3701,  2006, 14110,\n",
      "          2103,  1010,  2019,  6388, 14925,  3508,  1997,  1996, 26927,  2006,\n",
      "          2073,  2035,  1996,  3526,  2031,  1043,  2721,  2392,  1998,  2227,\n",
      "         20546,  1010,  1051,  9394,  1045,  2025,  2152,  2006,  1996, 11376,\n",
      "          1012,  7861,  2103,  1045,  2188,  2000,  2116,  1012,  1012, 26030,\n",
      "          1010, 14163, 18525,  1010,  6080, 11937,  1010,  7402,  1010, 10381,\n",
      "          3089, 23401,  1010,  3059,  1010, 20868,  2072,  1044,  1998,  2062,\n",
      "          1012,  1012,  1012,  1012,  1051, 26450,  2571,  1010,  2331, 16985,\n",
      "          2063,  1010, 26489,  6292,  7149,  1998,  2018,  2100,  3820,  2024,\n",
      "          2196,  2521,  2185,  1012,  1045,  2052,  1037,  2100,  1996,  2364,\n",
      "          5574,  1997,  1996,  2129,  1045,  2349,  2000,  1996,  2755,  2008,\n",
      "          2009,  2175,  2063,  2073,  2060,  2129,  2876,  1005,  1056,  8108,\n",
      "          1012,  5293,  3492,  3861,  4993,  2005,  2364, 29461,  3286,  4378,\n",
      "          1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012, 11472,\n",
      "         18629,  1050,  1005,  1056,  2033,  2105,  1012,  1996, 21554,  1056,\n",
      "          4958,  2072, 24040,  1045,  2412, 22091,  4744,  2033,  1037,  1051,\n",
      "          6583,  5939,  2009, 11333, 24471, 22852,  1010,  1045,  2481,  1005,\n",
      "          1056,  1037,  2100,  1045, 11333,  3201,  2005,  2009,  1010,  2021,\n",
      "          1037,  1045,  3427,  2062,  1010,  1045,  2764,  1037, 11937,  8915,\n",
      "          2005, 11472,  1010,  1998,  2288, 16222,  2226, 21269,  2094,  2000,\n",
      "          1996,  2152,  2504,  1997,  8425,  4808,  1012,  2025, 18414,  1056,\n",
      "          4808,  1010,  2021,  1999,  9103, 14841,  3401,  1006, 15274,  3457,\n",
      "          2040,  1005,  2222,  2022,  2214,  2041,  2005,  1037, 15519,  1010,\n",
      "         24467,  2040,  1005,  2222,  3102,  2006,  2344,  1998,  2131,  2185,\n",
      "          2007,  2009,  1010,  2092,  5450,  2098,  1010,  2690, 18856,  2050,\n",
      "         24467,  2108,  2357,  2046, 26927,  2006,  7743,  2063,  2349,  2000,\n",
      "          2037,  3768,  1997,  3392,  2102,  3102,  2030, 26927,  2006,  3325,\n",
      "          1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,  2007,  2054,\n",
      "          1045,  8796, 10523,  1012,  1012,  1012,  1012,  2008,  2065,  2017,\n",
      "          2064,  2131,  1999,  3543,  2007,  2115,  9904,  8909,  2063,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "token_type_ids:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "\n",
    "batch_encoder = tokenizer.encode_plus(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "print('Batch encoder keys:')\n",
    "print(batch_encoder.keys())\n",
    "\n",
    "print('\\nAttention mask:')\n",
    "print(batch_encoder['attention_mask'])\n",
    "\n",
    "print('\\ninput_ids:')\n",
    "print(batch_encoder['input_ids'])\n",
    "\n",
    "print('\\ntoken_type_ids:')\n",
    "print(batch_encoder['token_type_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df4ad34-b9e2-44d5-94eb-b5bcb26bf306",
   "metadata": {},
   "source": [
    "### **Encode All Reviews:**  \n",
    "토크나이징 단계의 마지막 단계는,  데이터셋에 포함된 모든 리뷰를 인코딩하고    \n",
    "그 결과로 얻은 토큰 ID와 어텐션 마스크를 텐서(tensor) 형식으로 저장하는 것이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e38c9fc-7f51-4768-99c6-adfcde45bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Encode each review\n",
    "for review in df['review_cleaned']:\n",
    "    batch_encoder = tokenizer.encode_plus(\n",
    "        review,\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_tensors = 'pt')\n",
    "\n",
    "    token_ids.append(batch_encoder['input_ids'])\n",
    "    attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "# Convert token IDs and attention mask lists to PyTorch tensors\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1657cb7e-d32f-4207-9a7f-4d2f24286506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.shape\n",
    "attention_masks.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
