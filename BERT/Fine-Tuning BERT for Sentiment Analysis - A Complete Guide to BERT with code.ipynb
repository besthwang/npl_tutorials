{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6b58f8-5b7e-430a-a144-4d3b272ab844",
   "metadata": {},
   "source": [
    "ì°¸ê³ ë¬¸í—Œ  \n",
    "\\[1\\] [A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e1fc6-ac1d-4b34-9536-64e4e228972d",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113ff54-5f4b-40ad-afef-9920bcd35d69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1 -Load and Preprocess a Fine-Tunning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcaa102a-993e-4f94-ad49-39c72eac4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d751d86-ff5e-48a2-a90f-acccb8343d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Datasets/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5c414-4413-4e2e-a517-5d323a9b2f86",
   "metadata": {},
   "source": [
    "ì´ì „ì˜ NLP ëª¨ë¸ë“¤ê³¼ëŠ” ë‹¬ë¦¬, BERTì™€ ê°™ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì€ ì „ì²˜ë¦¬ê°€ ê±°ì˜ í•„ìš”í•˜ì§€ ì•Šë‹¤.  \n",
    "ë¶ˆìš©ì–´(stop words)ë‚˜ êµ¬ë‘ì (punctuation)ì„ ì œê±°í•˜ëŠ” ë‹¨ê³„ëŠ” ê²½ìš°ì— ë”°ë¼ ì˜¤íˆë ¤ ì—­íš¨ê³¼ë¥¼ ë‚³ì„ ìˆ˜ ìˆë‹¤.  \n",
    "ì´ëŸ¬í•œ ìš”ì†Œë“¤ì´ ì…ë ¥ ë¬¸ì¥ì„ ì´í•´í•˜ëŠ” ë° ìœ ìš©í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ `BERT`ì—ê²Œ ì œê³µí•˜ê¸° ë•Œë¬¸ì´ë‹¤.  \n",
    "  \n",
    "ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , í…ìŠ¤íŠ¸ì— í¬ë§· ë¬¸ì œë‚˜ ì›ì¹˜ ì•ŠëŠ” íŠ¹ìˆ˜ ë¬¸ì ë“±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì‘ì—…ì€ ì—¬ì „íˆ ì¤‘ìš”í•˜ë‹¤.  \n",
    "ì „ë°˜ì ìœ¼ë¡œ IMDb ë°ì´í„°ì…‹ì€ ê½¤ ê¹¨ë—í•œ í¸ì´ì§€ë§Œ, ìŠ¤í¬ë˜í•‘ ê³¼ì •ì—ì„œ ìƒê¸´ ì¼ë¶€ í”ì ë“¤ì´ ë‚¨ì•„ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.  \n",
    ">ì˜ˆë¥¼ ë“¤ì–´:  \n",
    "HTML ì¤„ë°”ê¿ˆ íƒœê·¸ (\\<br />)  \n",
    "ë¶ˆí•„ìš”í•œ ê³µë°±(whitespace) ë“±  \n",
    "\n",
    "ì´ëŸ¬í•œ ìš”ì†Œë“¤ì€ ì œê±°í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702aefe3-c8ac-4de9-b1b9-34670c5458b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "A wonderful little production. <br /><br />The filming technique is very\n",
      "nAfter cleaning:\n",
      "A wonderful little production. The filming technique i  very una uming- \n"
     ]
    }
   ],
   "source": [
    "# Remove the break tags (<br />)\n",
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "# Remove unnecessary whitespace\n",
    "df['review_cleaned'] = df['review_cleaned'].replace('s+', ' ', regex=True)\n",
    "\n",
    "# Compare 72 characters of the second review before and after cleaning\n",
    "print('Before cleaning:')\n",
    "print(df.iloc[1]['review'][0:72])\n",
    "\n",
    "print('nAfter cleaning:')\n",
    "print(df.iloc[1]['review_cleaned'][0:72])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1198265-d113-4b52-aa6d-3261a340467d",
   "metadata": {},
   "source": [
    "### Encode the Sentiment:  \n",
    "ì „ì²˜ë¦¬ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ê° ë¦¬ë·°ì˜ ê°ì •ì„ ë¶€ì •ì¼ ê²½ìš° 0, ê¸ì •ì¼ ê²½ìš° 1ë¡œ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì´ë‹¤.  \n",
    "ì´ëŸ¬í•œ ë¼ë²¨ì€ íŒŒì¸íŠœë‹ ê³¼ì •ì—ì„œ ë¶„ë¥˜ í—¤ë“œ(classification head)ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë° ì‚¬ìš©ëœë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7fc76e-c359-4d53-b71a-a9312d57414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewer  ha  mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought thi  wa  a wonderful way to  pend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Ba ically there'  a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei'  \"Love in the Time of Money\" i ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      review_cleaned  sentiment_encoded  \n",
       "0  One of the other reviewer  ha  mentioned that ...                  1  \n",
       "1  A wonderful little production. The filming tec...                  1  \n",
       "2  I thought thi  wa  a wonderful way to  pend ti...                  1  \n",
       "3  Ba ically there'  a family where a little boy ...                  0  \n",
       "4  Petter Mattei'  \"Love in the Time of Money\" i ...                  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bce89-aa0d-43c0-b68f-40d8944c17a3",
   "metadata": {},
   "source": [
    "## 3.2 â€“ íŒŒì¸íŠœë‹ ë°ì´í„° í† í¬ë‚˜ì´ì¦ˆ(Tokenize)\n",
    "ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´, íŒŒì¸íŠœë‹ ë°ì´í„°ì— ëŒ€í•´ í† í¬ë‚˜ì´ì¦ˆ(`tokenization`) ê³¼ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.  \n",
    "ì´ ê³¼ì •ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì´ ì´ë£¨ì–´ì§„ë‹¤:  \n",
    "\n",
    "ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ê°œë³„ í† í°(token) ìœ¼ë¡œ ë¶„í•   \n",
    "[CLS]ì™€ [SEP] ê°™ì€ íŠ¹ìˆ˜ í† í° ì¶”ê°€  \n",
    "\n",
    "ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•œ íŒ¨ë”© ì²˜ë¦¬  \n",
    "\n",
    "ëª¨ë¸ë§ˆë‹¤ ìš”êµ¬í•˜ëŠ” í† í¬ë‚˜ì´ì§• ë°©ì‹ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, í•´ë‹¹ ëª¨ë¸ì— ë§ëŠ” ì ì ˆí•œ í† í¬ë‚˜ì´ì €ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.  \n",
    "ì˜ˆë¥¼ ë“¤ì–´, GPTëŠ” [CLS] ë° [SEP] í† í°ì„ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, BERTëŠ” ì´ë¥¼ ì‚¬ìš©í•œë‹¤.  \n",
    "\n",
    "ì´ë²ˆ ì˜ˆì œì—ì„œëŠ” `Hugging Face`ì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” `BertTokenizer` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤.  \n",
    "ì´ í† í¬ë‚˜ì´ì €ëŠ” `BERT` ê³„ì—´ ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆë‹¤.  \n",
    "í† í¬ë‚˜ì´ì§• ì‘ë™ ë°©ì‹ì— ëŒ€í•œ ë³´ë‹¤ ìì„¸í•œ ì„¤ëª…ì€ ì´ ì‹œë¦¬ì¦ˆì˜ Part 1ì„ ì°¸ê³ í•˜ë©´ ëœë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6eab0-a18e-4506-9929-3c8dece6da30",
   "metadata": {},
   "source": [
    "transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ëŠ” from_pretrained ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬  \n",
    "**ì‚¬ì „í•™ìŠµëœ í† í¬ë‚˜ì´ì € ëª¨ë¸ì„ ì†ì‰½ê²Œ ìƒì„±**í•  ìˆ˜ ìˆë‹¤.  \n",
    "ì´ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:  \n",
    "\n",
    "1. í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ë¥¼ import ë° ì¸ìŠ¤í„´ìŠ¤í™”  \n",
    "2. from_pretrained ë©”ì„œë“œë¥¼ í˜¸ì¶œ  \n",
    "3. Hugging Face ëª¨ë¸ ì €ì¥ì†Œì— ìˆëŠ” í† í¬ë‚˜ì´ì € ì´ë¦„(ë¬¸ìì—´ í˜•íƒœ)ì„ ì¸ìë¡œ ì „ë‹¬\n",
    "  \n",
    "ë˜ëŠ”, ë¡œì»¬ì— ì €ì¥ëœ ë‹¨ì–´ ì§‘í•©(vocabulary) íŒŒì¼ì´ í¬í•¨ëœ ë””ë ‰í„°ë¦¬ ê²½ë¡œë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤ [9].  \n",
    "\n",
    "---  \n",
    "\n",
    "ì´ë²ˆ ì˜ˆì œì—ì„œëŠ” Hugging Face ëª¨ë¸ ì €ì¥ì†Œì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „í•™ìŠµëœ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë©°,  \n",
    "BERT ê´€ë ¨ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë„¤ ê°€ì§€ ì£¼ìš” ì˜µì…˜ì´ ìˆë‹¤.   \n",
    "ì´ë“¤ì€ ëª¨ë‘ Googleì˜ ì‚¬ì „í•™ìŠµ BERT í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤:  \n",
    "\n",
    "- bert-base-uncased  \n",
    "â†’ BERT Base ëª¨ë¸ìš©, ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆ í•¨ (ì˜ˆ: Catê³¼ catì„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)\n",
    "\n",
    "- bert-base-cased  \n",
    "â†’ BERT Base ëª¨ë¸ìš©, ëŒ€ì†Œë¬¸ì êµ¬ë¶„í•¨ (ì˜ˆ: Catê³¼ catì„ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)\n",
    "\n",
    "- bert-large-uncased  \n",
    "â†’ BERT Large ëª¨ë¸ìš©, ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆ í•¨\n",
    "\n",
    "- bert-large-cased  \n",
    "â†’ BERT Large ëª¨ë¸ìš©, ëŒ€ì†Œë¬¸ì êµ¬ë¶„í•¨\n",
    "\n",
    "`BERT Base`ì™€ `BERT Large`ëŠ” **ë™ì¼í•œ ì–´íœ˜ ì§‘í•©(vocabulary)** ì„ ì‚¬ìš©í•˜ë¯€ë¡œ,  \n",
    "`bert-base-uncased`ì™€ `bert-large-uncased`ëŠ” ì‹¤ì§ˆì ìœ¼ë¡œ ì°¨ì´ê°€ ì—†ê³ ,  \n",
    "`bert-base-cased`ì™€ `bert-large-cased`ë„ ë™ì¼í•œ ì–´íœ˜ë¥¼ ì‚¬ìš©í•œë‹¤.  \n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì—ì„œëŠ” ë™ì¼í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í™•ì‹¤í•˜ì§€ ì•Šë‹¤ë©´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ì˜  \n",
    "í¬ê¸°ë¥¼ ì¼ì¹˜ì‹œì¼œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì „í•˜ë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0b138-18a5-4ee6-836e-a22d22029bf4",
   "metadata": {},
   "source": [
    "### ëŒ€ì†Œë¬¸ì êµ¬ë¶„(cased vs uncased)ì„ ì–¸ì œ ì‚¬ìš©í• ê¹Œ?\n",
    "(When to Use cased vs uncased:)  \n",
    "\n",
    "cased ëª¨ë¸ê³¼ uncased ëª¨ë¸ ì¤‘ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ì§€ëŠ” ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.  \n",
    "ì˜ˆë¥¼ ë“¤ì–´, IMDb ë°ì´í„°ì…‹ì€ ì¸í„°ë„· ì‚¬ìš©ìë“¤ì´ ì‘ì„±í•œ ë¦¬ë·°ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.  \n",
    "ì´ë“¤ì€ ëŒ€ì†Œë¬¸ì ì‚¬ìš©ì— ì¼ê´€ì„±ì´ ì—†ì„ ìˆ˜ ìˆë‹¤.  \n",
    "\n",
    "ì–´ë–¤ ì‚¬ìš©ìëŠ” ë¬¸ì¥ì˜ ì²« ê¸€ìì—ì„œ ëŒ€ë¬¸ìë¥¼ ìƒëµí•˜ê¸°ë„ í•˜ê³ ,  \n",
    "ì–´ë–¤ ì‚¬ìš©ìëŠ” **ê°•ì¡°ë‚˜ ê°ì • í‘œí˜„(í¥ë¶„, ë¶„ë…¸ ë“±)**ì„ ìœ„í•´ ëŒ€ë¬¸ìë¥¼ ê³¼í•˜ê²Œ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤.  \n",
    "ì´ëŸ¬í•œ ì´ìœ ë¡œ ìš°ë¦¬ëŠ” ëŒ€ì†Œë¬¸ìë¥¼ ë¬´ì‹œ(uncased) í•˜ê¸°ë¡œ ê²°ì •í•˜ê³ ,  \n",
    "bert-base-uncased í† í¬ë‚˜ì´ì € ëª¨ë¸ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.  \n",
    "\n",
    "í•˜ì§€ë§Œ, ê²½ìš°ì— ë”°ë¼ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ëŠ”(cased) ê²ƒì´ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ê¸°ë„ í•œë‹¤.  \n",
    "ëŒ€í‘œì ì¸ ì˜ˆë¡œëŠ” ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER) ê³¼ì œê°€ ìˆë‹¤.  \n",
    "ì´ ê³¼ì œì—ì„œëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì‚¬ëŒ, ì¡°ì§, ì¥ì†Œ ë“±ì˜ **ê³ ìœ  ëª…ì‚¬(Entity)** ë¥¼ ì‹ë³„í•˜ëŠ” ê²ƒì´ ëª©í‘œì¸ë°,  \n",
    "ì´ ê²½ìš° ëŒ€ë¬¸ìì˜ ì¡´ì¬ ì—¬ë¶€ëŠ” í•´ë‹¹ ë‹¨ì–´ê°€ ì‚¬ëŒ ì´ë¦„ì¸ì§€, ì¥ì†Œ ì´ë¦„ì¸ì§€ ë“±ì„ íŒë³„í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•˜ë‹¤.  \n",
    "\n",
    "ë”°ë¼ì„œ, ì´ëŸ¬í•œ ìƒí™©ì—ì„œëŠ” bert-base-cased ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•  ìˆ˜ ìˆë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32157f09-80b9-46c1-b7f3-b26d29573319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2b61c-fb43-446d-9744-e42542cf1dcb",
   "metadata": {},
   "source": [
    "### **Encoding Process: Converting Text to Tokens to Token IDs**\n",
    "\n",
    "ì œ ì „ì²˜ë¦¬ëœ íŒŒì¸íŠœë‹ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•  ì°¨ë¡€ë‹¤.  \n",
    "ì´ ê³¼ì •ì—ì„œëŠ” ê° ë¦¬ë·°ê°€ **í† í° IDë¡œ ì´ë£¨ì–´ì§„ í…ì„œ(tensor)**ë¡œ ë³€í™˜ëœë‹¤.  \n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ë¦¬ë·° \"I liked this movie\" ëŠ” ë‹¤ìŒ ë‹¨ê³„ë¥¼ í†µí•´ ì¸ì½”ë”©ëœë‹¤:  \n",
    "\n",
    "1. ì†Œë¬¸ì ë³€í™˜  \n",
    "â†’ ìš°ë¦¬ê°€ bert-base-uncasedë¥¼ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë¯€ë¡œ, ì…ë ¥ ë¬¸ì¥ì€ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜ëœë‹¤.  \n",
    "\n",
    "2. í† í° ë¶„í•   \n",
    "â†’ BERT ì–´íœ˜(bert-base-uncasedì˜ vocabulary)ì— ë”°ë¼ ë¬¸ì¥ì„ í† í° ë‹¨ìœ„ë¡œ ë¶„í• :  \n",
    "['i', 'liked', 'this', 'movie']  \n",
    "\n",
    "3. íŠ¹ìˆ˜ í† í° ì¶”ê°€  \n",
    "â†’ BERTì—ì„œ ê¸°ëŒ€í•˜ëŠ” íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€:  \n",
    "['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']  \n",
    "\n",
    "4. í† í°ì„ í† í° IDë¡œ ë³€í™˜  \n",
    "â†’ ê° í† í°ì„ BERT ì–´íœ˜ì— ë”°ë¼ ì •ìˆ˜í˜• IDë¡œ ë§¤í•‘ (ì˜ˆ: [CLS] â†’ 101, i â†’ 1045 ë“±)  \n",
    "---\n",
    "ì´ ì „ì²´ ì¸ì½”ë”© ê³¼ì •ì€ `BertTokenizer` í´ë˜ìŠ¤ì˜ `encode` ë©”ì„œë“œë¥¼ í†µí•´ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.  \n",
    "ì´ ë©”ì„œë“œëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ì—¬ í† í° ID í…ì„œë¥¼ ë°˜í™˜í•˜ë©°, ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ ê°€ëŠ¥í•˜ë‹¤:  \n",
    "\n",
    "- PyTorch í…ì„œ (pt)\n",
    "- TensorFlow í…ì„œ (tf)\n",
    "- NumPy ë°°ì—´ (np)\n",
    "\n",
    "ë°˜í™˜ í˜•ì‹ì€ return_tensors ì¸ìë¥¼ í†µí•´ ì§€ì •í•  ìˆ˜ ìˆë‹¤.  \n",
    "\n",
    "> ğŸ’¡ ì°¸ê³ : Hugging Faceì—ì„œëŠ” Token IDë¥¼ ì¢…ì¢… Input IDë¼ê³  ë¶€ë¥´ê¸°ë„ í•˜ë©°, ë‘ ìš©ì–´ëŠ” í˜¼ìš©ë˜ì–´ ì‚¬ìš©ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe744420-0c9f-45f6-9fe5-b6735c457a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [ 101 1045 4669 2023 3185  102]\n",
      "Tokens   : ['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Encode a sample input sentence\n",
    "sample_sentence = 'I liked this movie'\n",
    "token_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\n",
    "print(f'Token IDs: {token_ids}')\n",
    "\n",
    "# Convert the token IDs back to tokens to reveal the special tokens added\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f'Tokens   : {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d97b1d-f442-4cad-a35a-0458e72f7493",
   "metadata": {},
   "source": [
    "### **Truncation and Padding:**\n",
    "\n",
    "BERT Baseì™€ BERT LargeëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ì •í™•íˆ 512ê°œ í† í°ì¼ ë•Œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆë‹¤.  \n",
    "ê·¸ë ‡ë‹¤ë©´ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ì´ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ ë¶€ì¡±í•  ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?  \n",
    "ì •ë‹µì€ ë°”ë¡œ **ì˜ë¼ë‚´ê¸°(truncation)** ì™€ **íŒ¨ë”©(padding)** ì´ë‹¤!  \n",
    "\n",
    "**ì˜ë¼ë‚´ê¸° (Truncation):**    \n",
    "ì˜ë¼ë‚´ê¸°ëŠ” ì§€ì •ëœ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” í† í°ë“¤ì„ ë‹¨ìˆœíˆ ì˜ë¼ì„œ ì œê±°í•˜ëŠ” ë°©ì‹ì´ë‹¤.  \n",
    "encode ë©”ì„œë“œì—ì„œ truncation=Trueë¡œ ì„¤ì •í•˜ê³ , max_length ì¸ìë¥¼ ì§€ì •í•˜ë©´  \n",
    "ëª¨ë“  ì¸ì½”ë”©ëœ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ê¸¸ì´ ì œí•œì„ ê°•ì œí•  ìˆ˜ ìˆë‹¤.  \n",
    "\n",
    "ì´ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ ë¦¬ë·°ëŠ” 512ê°œ í† í° ì œí•œì„ ì´ˆê³¼í•˜ë¯€ë¡œ,  \n",
    "ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆë„ë¡ max_length=512ë¡œ ì„¤ì •í•œë‹¤.  \n",
    "ë§Œì•½ ëª¨ë“  ë¦¬ë·°ê°€ 512 í† í°ì„ ë„˜ì§€ ì•ŠëŠ”ë‹¤ë©´, max_lengthëŠ” ìƒëµí•´ë„ ë˜ê³ ,  \n",
    "ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´(512)**ê°€ ìë™ìœ¼ë¡œ ì ìš©ëœë‹¤.  \n",
    "\n",
    "ë˜ëŠ”, í•™ìŠµ ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ 512ë³´ë‹¤ ì§§ì€ ê¸¸ì´ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.  \n",
    "ë‹¤ë§Œ, ì´ ê²½ìš° ëª¨ë¸ ì„±ëŠ¥ì´ ë‹¤ì†Œ í¬ìƒë  ìˆ˜ ìˆìŒì— ìœ ì˜í•´ì•¼ í•œë‹¤.  \n",
    "\n",
    "**íŒ¨ë”© (Padding):**    \n",
    "ëŒ€ë¶€ë¶„ì˜ ë¦¬ë·°ëŠ” 512 í† í°ë³´ë‹¤ ì§§ê¸° ë•Œë¬¸ì—,  \n",
    "ì´ëŸ° ê²½ìš°ì—ëŠ” [PAD] í† í°ì„ ì¶”ê°€í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ 512 í† í°ê¹Œì§€ í™•ì¥í•œë‹¤.  \n",
    "ì´ë¥¼ ìœ„í•´ padding='max_length' ì˜µì…˜ì„ ì„¤ì •í•˜ë©´ ëœë‹¤.  \n",
    "\n",
    "ìì„¸í•œ ë‚´ìš©ì€ Hugging Faceì˜ encode ë©”ì„œë“œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì [10].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff22a2b-1020-4a51-9f6f-c5da3b94281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review  example.\n",
      "One of the other reviewer  ha  mentioned that after watching ju t 1 Oz epi ode you'll be hooked. They are right, a  thi  i  exactly what happened with me.The fir t thing that  truck me about Oz wa  it  brutality and unflinching  cene  of violence, which  et in right from the word GO. Tru t me, thi  i  not a  how for the faint hearted or timid. Thi   how pull  no punche  with regard  to drug ,  ex or violence. It  i  hardcore, in the cla ic u e of the word.It i  called OZ a  that i  the nickname given to the O wald Maximum Security State Penitentary. It focu e  mainly on Emerald City, an experimental  ection of the pri on where all the cell  have gla  front  and face inward ,  o privacy i  not high on the agenda. Em City i  home to many..Aryan , Mu lim , gang ta , Latino , Chri tian , Italian , Iri h and more.... o  cuffle , death  tare , dodgy dealing  and  hady agreement  are never far away.I would  ay the main appeal of the  how i  due to the fact that it goe  where other  how  wouldn't dare. Forget pretty picture  painted for main tream audience , forget charm, forget romance...OZ doe n't me  around. The fir t epi ode I ever  aw  truck me a   o na ty it wa   urreal, I couldn't  ay I wa  ready for it, but a  I watched more, I developed a ta te for Oz, and got accu tomed to the high level  of graphic violence. Not ju t violence, but inju tice (crooked guard  who'll be  old out for a nickel, inmate  who'll kill on order and get away with it, well mannered, middle cla  inmate  being turned into pri on bitche  due to their lack of  treet  kill  or pri on experience) Watching Oz, you may become comfortable with what i  uncomfortable viewing....that  if you can get in touch with your darker  ide.\n",
      "----------------------\n",
      "tensor([[  101,  2028,  1997,  1996,  2060, 12027,  5292,  3855,  2008,  2044,\n",
      "          3666, 18414,  1056,  1015, 11472,  4958,  2072, 24040,  2017,  1005,\n",
      "          2222,  2022, 13322,  1012,  2027,  2024,  2157,  1010,  1037, 16215,\n",
      "          2072,  1045,  3599,  2054,  3047,  2007,  2033,  1012,  1996, 21554,\n",
      "          1056,  2518,  2008,  4744,  2033,  2055, 11472, 11333,  2009, 24083,\n",
      "          1998,  4895, 10258,  2378,  8450,  8292,  2638,  1997,  4808,  1010,\n",
      "          2029,  3802,  1999,  2157,  2013,  1996,  2773,  2175,  1012, 19817,\n",
      "          2226,  1056,  2033,  1010, 16215,  2072,  1045,  2025,  1037,  2129,\n",
      "          2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012, 16215,  2072,\n",
      "          2129,  4139,  2053,  8595,  2063,  2007,  7634,  2000,  4319,  1010,\n",
      "          4654,  2030,  4808,  1012,  2009,  1045, 13076,  1010,  1999,  1996,\n",
      "         18856,  2050, 24582,  1057,  1041,  1997,  1996,  2773,  1012,  2009,\n",
      "          1045,  2170, 11472,  1037,  2008,  1045,  1996,  8367,  2445,  2000,\n",
      "          1996,  1051, 24547,  2094,  4555,  3036,  2110,  7279,  4221, 12380,\n",
      "          2854,  1012,  2009,  1042, 10085,  2226,  1041,  3701,  2006, 14110,\n",
      "          2103,  1010,  2019,  6388, 14925,  3508,  1997,  1996, 26927,  2006,\n",
      "          2073,  2035,  1996,  3526,  2031,  1043,  2721,  2392,  1998,  2227,\n",
      "         20546,  1010,  1051,  9394,  1045,  2025,  2152,  2006,  1996, 11376,\n",
      "          1012,  7861,  2103,  1045,  2188,  2000,  2116,  1012,  1012, 26030,\n",
      "          1010, 14163, 18525,  1010,  6080, 11937,  1010,  7402,  1010, 10381,\n",
      "          3089, 23401,  1010,  3059,  1010, 20868,  2072,  1044,  1998,  2062,\n",
      "          1012,  1012,  1012,  1012,  1051, 26450,  2571,  1010,  2331, 16985,\n",
      "          2063,  1010, 26489,  6292,  7149,  1998,  2018,  2100,  3820,  2024,\n",
      "          2196,  2521,  2185,  1012,  1045,  2052,  1037,  2100,  1996,  2364,\n",
      "          5574,  1997,  1996,  2129,  1045,  2349,  2000,  1996,  2755,  2008,\n",
      "          2009,  2175,  2063,  2073,  2060,  2129,  2876,  1005,  1056,  8108,\n",
      "          1012,  5293,  3492,  3861,  4993,  2005,  2364, 29461,  3286,  4378,\n",
      "          1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012, 11472,\n",
      "         18629,  1050,  1005,  1056,  2033,  2105,  1012,  1996, 21554,  1056,\n",
      "          4958,  2072, 24040,  1045,  2412, 22091,  4744,  2033,  1037,  1051,\n",
      "          6583,  5939,  2009, 11333, 24471, 22852,  1010,  1045,  2481,  1005,\n",
      "          1056,  1037,  2100,  1045, 11333,  3201,  2005,  2009,  1010,  2021,\n",
      "          1037,  1045,  3427,  2062,  1010,  1045,  2764,  1037, 11937,  8915,\n",
      "          2005, 11472,  1010,  1998,  2288, 16222,  2226, 21269,  2094,  2000,\n",
      "          1996,  2152,  2504,  1997,  8425,  4808,  1012,  2025, 18414,  1056,\n",
      "          4808,  1010,  2021,  1999,  9103, 14841,  3401,  1006, 15274,  3457,\n",
      "          2040,  1005,  2222,  2022,  2214,  2041,  2005,  1037, 15519,  1010,\n",
      "         24467,  2040,  1005,  2222,  3102,  2006,  2344,  1998,  2131,  2185,\n",
      "          2007,  2009,  1010,  2092,  5450,  2098,  1010,  2690, 18856,  2050,\n",
      "         24467,  2108,  2357,  2046, 26927,  2006,  7743,  2063,  2349,  2000,\n",
      "          2037,  3768,  1997,  3392,  2102,  3102,  2030, 26927,  2006,  3325,\n",
      "          1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,  2007,  2054,\n",
      "          1045,  8796, 10523,  1012,  1012,  1012,  1012,  2008,  2065,  2017,\n",
      "          2064,  2131,  1999,  3543,  2007,  2115,  9904,  8909,  2063,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    }
   ],
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "\n",
    "print(\"review  example.\");\n",
    "print(review)\n",
    "\n",
    "print(\"----------------------\")\n",
    "\n",
    "token_ids = tokenizer.encode(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143c3c2-ecbc-4483-8edc-3307ecb5db33",
   "metadata": {},
   "source": [
    "### `encode_plus`ì™€ í•¨ê»˜ Attention Mask ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "ì•ì„œ ì‚´í´ë³¸ ì˜ˆì œì—ì„œëŠ” ë°ì´í„°ì…‹ì˜ ì²« ë²ˆì§¸ ë¦¬ë·°ê°€ ì¸ì½”ë”©ëœ ê²°ê³¼, `119`ê°œì˜ íŒ¨ë”© í† í°ì„ í¬í•¨í•˜ê³  ìˆë‹¤.  \n",
    "ì´ ìƒíƒœ ê·¸ëŒ€ë¡œ íŒŒì¸íŠœë‹ì— ì‚¬ìš©í•˜ë©´, BERTê°€ íŒ¨ë”© í† í°ì—ê¹Œì§€ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ê²Œ ë˜ì–´  \n",
    "ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤.  \n",
    "\n",
    "ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” **ì–´í…ì…˜ ë§ˆìŠ¤í¬(attention mask)** ë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤.  \n",
    "ì–´í…ì…˜ ë§ˆìŠ¤í¬ëŠ” BERTì—ê²Œ ì…ë ¥ì—ì„œ íŠ¹ì • í† í°(ì´ ê²½ìš° íŒ¨ë”© í† í°)ì„ ë¬´ì‹œí•˜ë¼ê³  ì§€ì‹œí•˜ëŠ” ì—­í• ì„ í•œë‹¤.    \n",
    "\n",
    "-----\n",
    "\n",
    "ì´ë¥¼ êµ¬í˜„í•˜ë ¤ë©´ ê¸°ì¡´ì˜ encode ë©”ì„œë“œ ëŒ€ì‹ ,  \n",
    "encode_plus ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©´ ëœë‹¤.  \n",
    "\n",
    "encode_plusëŠ” Hugging Faceì—ì„œ **ë°°ì¹˜ ì¸ì½”ë”(Batch Encoder)** ë¼ê³  ë¶ˆë¦¬ëŠ”  \n",
    "ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ê°’ì„ ë°˜í™˜í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì€ í‚¤ë¥¼ í¬í•¨í•œë‹¤:  \n",
    "\n",
    "- **input_ids**:  \n",
    "â†’ encode ë©”ì„œë“œì™€ ë™ì¼í•˜ê²Œ í† í° IDë“¤ì„ ë°˜í™˜  \n",
    "\n",
    "- **token_type_ids**:  \n",
    "â†’ ë¬¸ì¥ ìŒ ê³¼ì œ(Natural Language Inference, Next Sentence Prediction ë“±)ì—ì„œ\n",
    "ë¬¸ì¥ A(ê°’ = 0)ì™€ ë¬¸ì¥ B(ê°’ = 1)ë¥¼ êµ¬ë¶„í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ID\n",
    "\n",
    "- **attention_mask**:  \n",
    "â†’ 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§„ ë¦¬ìŠ¤íŠ¸\n",
    "â†’ 0ì€ í•´ë‹¹ í† í°ì´ ì–´í…ì…˜ì—ì„œ ë¬´ì‹œë˜ì–´ì•¼ í•¨ì„ ì˜ë¯¸ (ì˜ˆ: íŒ¨ë”© í† í°)  \n",
    "â†’ 1ì€ í•´ë‹¹ í† í°ì´ ì–´í…ì…˜ì—ì„œ ì‚¬ìš©ë˜ì–´ì•¼ í•¨ì„ ì˜ë¯¸  \n",
    "\n",
    "ì´ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í†µí•´ BERTëŠ” ì‹¤ì œ ì˜ë¯¸ ìˆëŠ” í† í°ì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ ë˜ë©°,  \n",
    "ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ìœ ì§€ë˜ê±°ë‚˜ í–¥ìƒë  ìˆ˜ ìˆë‹¤.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894ef5ac-3164-4c11-861d-d9f2c32f9b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch encoder keys:\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "Attention mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "input_ids:\n",
      "tensor([[  101,  2028,  1997,  1996,  2060, 12027,  5292,  3855,  2008,  2044,\n",
      "          3666, 18414,  1056,  1015, 11472,  4958,  2072, 24040,  2017,  1005,\n",
      "          2222,  2022, 13322,  1012,  2027,  2024,  2157,  1010,  1037, 16215,\n",
      "          2072,  1045,  3599,  2054,  3047,  2007,  2033,  1012,  1996, 21554,\n",
      "          1056,  2518,  2008,  4744,  2033,  2055, 11472, 11333,  2009, 24083,\n",
      "          1998,  4895, 10258,  2378,  8450,  8292,  2638,  1997,  4808,  1010,\n",
      "          2029,  3802,  1999,  2157,  2013,  1996,  2773,  2175,  1012, 19817,\n",
      "          2226,  1056,  2033,  1010, 16215,  2072,  1045,  2025,  1037,  2129,\n",
      "          2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012, 16215,  2072,\n",
      "          2129,  4139,  2053,  8595,  2063,  2007,  7634,  2000,  4319,  1010,\n",
      "          4654,  2030,  4808,  1012,  2009,  1045, 13076,  1010,  1999,  1996,\n",
      "         18856,  2050, 24582,  1057,  1041,  1997,  1996,  2773,  1012,  2009,\n",
      "          1045,  2170, 11472,  1037,  2008,  1045,  1996,  8367,  2445,  2000,\n",
      "          1996,  1051, 24547,  2094,  4555,  3036,  2110,  7279,  4221, 12380,\n",
      "          2854,  1012,  2009,  1042, 10085,  2226,  1041,  3701,  2006, 14110,\n",
      "          2103,  1010,  2019,  6388, 14925,  3508,  1997,  1996, 26927,  2006,\n",
      "          2073,  2035,  1996,  3526,  2031,  1043,  2721,  2392,  1998,  2227,\n",
      "         20546,  1010,  1051,  9394,  1045,  2025,  2152,  2006,  1996, 11376,\n",
      "          1012,  7861,  2103,  1045,  2188,  2000,  2116,  1012,  1012, 26030,\n",
      "          1010, 14163, 18525,  1010,  6080, 11937,  1010,  7402,  1010, 10381,\n",
      "          3089, 23401,  1010,  3059,  1010, 20868,  2072,  1044,  1998,  2062,\n",
      "          1012,  1012,  1012,  1012,  1051, 26450,  2571,  1010,  2331, 16985,\n",
      "          2063,  1010, 26489,  6292,  7149,  1998,  2018,  2100,  3820,  2024,\n",
      "          2196,  2521,  2185,  1012,  1045,  2052,  1037,  2100,  1996,  2364,\n",
      "          5574,  1997,  1996,  2129,  1045,  2349,  2000,  1996,  2755,  2008,\n",
      "          2009,  2175,  2063,  2073,  2060,  2129,  2876,  1005,  1056,  8108,\n",
      "          1012,  5293,  3492,  3861,  4993,  2005,  2364, 29461,  3286,  4378,\n",
      "          1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012, 11472,\n",
      "         18629,  1050,  1005,  1056,  2033,  2105,  1012,  1996, 21554,  1056,\n",
      "          4958,  2072, 24040,  1045,  2412, 22091,  4744,  2033,  1037,  1051,\n",
      "          6583,  5939,  2009, 11333, 24471, 22852,  1010,  1045,  2481,  1005,\n",
      "          1056,  1037,  2100,  1045, 11333,  3201,  2005,  2009,  1010,  2021,\n",
      "          1037,  1045,  3427,  2062,  1010,  1045,  2764,  1037, 11937,  8915,\n",
      "          2005, 11472,  1010,  1998,  2288, 16222,  2226, 21269,  2094,  2000,\n",
      "          1996,  2152,  2504,  1997,  8425,  4808,  1012,  2025, 18414,  1056,\n",
      "          4808,  1010,  2021,  1999,  9103, 14841,  3401,  1006, 15274,  3457,\n",
      "          2040,  1005,  2222,  2022,  2214,  2041,  2005,  1037, 15519,  1010,\n",
      "         24467,  2040,  1005,  2222,  3102,  2006,  2344,  1998,  2131,  2185,\n",
      "          2007,  2009,  1010,  2092,  5450,  2098,  1010,  2690, 18856,  2050,\n",
      "         24467,  2108,  2357,  2046, 26927,  2006,  7743,  2063,  2349,  2000,\n",
      "          2037,  3768,  1997,  3392,  2102,  3102,  2030, 26927,  2006,  3325,\n",
      "          1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,  2007,  2054,\n",
      "          1045,  8796, 10523,  1012,  1012,  1012,  1012,  2008,  2065,  2017,\n",
      "          2064,  2131,  1999,  3543,  2007,  2115,  9904,  8909,  2063,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "token_type_ids:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "\n",
    "batch_encoder = tokenizer.encode_plus(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "print('Batch encoder keys:')\n",
    "print(batch_encoder.keys())\n",
    "\n",
    "print('\\nAttention mask:')\n",
    "print(batch_encoder['attention_mask'])\n",
    "\n",
    "print('\\ninput_ids:')\n",
    "print(batch_encoder['input_ids'])\n",
    "\n",
    "print('\\ntoken_type_ids:')\n",
    "print(batch_encoder['token_type_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df4ad34-b9e2-44d5-94eb-b5bcb26bf306",
   "metadata": {},
   "source": [
    "### **Encode All Reviews:**  \n",
    "í† í¬ë‚˜ì´ì§• ë‹¨ê³„ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ”,  ë°ì´í„°ì…‹ì— í¬í•¨ëœ ëª¨ë“  ë¦¬ë·°ë¥¼ ì¸ì½”ë”©í•˜ê³     \n",
    "ê·¸ ê²°ê³¼ë¡œ ì–»ì€ í† í° IDì™€ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í…ì„œ(tensor) í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ê²ƒì´ë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e38c9fc-7f51-4768-99c6-adfcde45bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Encode each review\n",
    "for review in df['review_cleaned']:\n",
    "    batch_encoder = tokenizer.encode_plus(\n",
    "        review,\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_tensors = 'pt')\n",
    "\n",
    "    token_ids.append(batch_encoder['input_ids'])\n",
    "    attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "# Convert token IDs and attention mask lists to PyTorch tensors\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1657cb7e-d32f-4207-9a7f-4d2f24286506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.shape\n",
    "attention_masks.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
