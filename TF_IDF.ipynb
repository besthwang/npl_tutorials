{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ffa5dd-a0bb-4dd8-8028-32f0630eceb8",
   "metadata": {},
   "source": [
    "지금부터는 코드를 통해 TF-IDF를 실습해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2197e9bd-97f2-4ab5-974b-812507d4b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I am a great great elementary school student\", \"And I am a boy\"]\n",
    "\n",
    "# Word tokenized sentence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_tokenzied = [word_tokenize(sentence) for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bf7cd6-9f1d-49f6-80b6-7e07ebd14d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'am', 'a', 'great', 'great', 'elementary', 'school', 'student'],\n",
       " ['And', 'I', 'am', 'a', 'boy']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenzied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113a37f-eac8-4b40-a91b-8952590bf79f",
   "metadata": {},
   "source": [
    "처음에 문서인 text를 정의해줍니다. 여기서 우리는 각 문장을 문서라고 정의하겠습니다.  \n",
    "즉 text에는 문서 1, 문서 2 이렇게 2개의 문서가 있는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6702dd8-f41d-44cc-800d-0102651d36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: I:1\n",
      "word: am:2\n",
      "word: a:1\n",
      "word: great:5\n",
      "word: great:5\n",
      "word: elementary:10\n",
      "word: school:6\n",
      "word: student:7\n",
      "word: And:3\n",
      "word: I:1\n",
      "word: am:2\n",
      "word: a:1\n",
      "word: boy:3\n",
      "[['am', 'great', 'great', 'elementary', 'school', 'student'], ['And', 'am', 'boy']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords that are two short.\n",
    "\n",
    "text_tokenzied2 = []\n",
    "for sentence in text_tokenzied:\n",
    "    sent = []\n",
    "    for word in sentence:\n",
    "        print(f\"word: {word}:{len(word)}\")\n",
    "        if len(word) >=2:\n",
    "            sent.append(word)\n",
    "    text_tokenzied2.append(sent)\n",
    "        \n",
    "print(text_tokenzied2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae395ddd-6bb4-4a70-894b-916a1b8cac6f",
   "metadata": {},
   "source": [
    "그 다음 학습의 성능을 향상하기 위해 글자 수가 하나인 단어를 제거해줍니다.   \n",
    "토크 나이저 된 리스트는 위와 같이 문서들에서 단어 'I'가 사라진것을  \n",
    "알 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a402f81-4479-47c5-8b1b-e9ef5e6672b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am 2\n",
      "great 2\n",
      "elementary 1\n",
      "school 1\n",
      "student 1\n",
      "And 1\n",
      "boy 1\n",
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary list\n",
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "for sentence in text_tokenzied2:\n",
    "    vocab_counter.update(sentence)\n",
    "\n",
    "vocab = []\n",
    "for key, value in vocab_counter.items():\n",
    "    print(key, value)\n",
    "    vocab.append(key)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efcf04-247f-4fe3-b5dc-cdda7e55139a",
   "metadata": {},
   "source": [
    "전체 문서들의 중복제거한 단어들을 list로 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e689b1-5c8b-4aaa-9887-aca471ee5525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'great': 2, 'am': 1, 'elementary': 1, 'school': 1, 'student': 1}), Counter({'And': 1, 'am': 1, 'boy': 1})]\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for sentence in text_tokenzied2:\n",
    "    vocab_counter = Counter()\n",
    "    vocab_counter.update(sentence)\n",
    "    count.append(vocab_counter)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a9f04-d307-4626-915c-9f3dcff87281",
   "metadata": {},
   "source": [
    "- TF 계산에 사용되는 단어는 전체 문서에 있는 단어를 대상으로 함.  \n",
    "- vocab는 전체 문서에 있는 단어의 리스트임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5918ef6e-7df8-41a0-a9f6-ca308caf6619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[1, 2, 1, 1, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def TF(vocab, counter):\n",
    "    vector = []\n",
    "    for word in vocab:\n",
    "        if counter[word] != False:\n",
    "            vector.append(counter[word])\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "\n",
    "print(vocab)\n",
    "print(TF(vocab, count[0]))\n",
    "print(TF(vocab, count[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02178a-db66-4c23-9ebc-be3d90b191b6",
   "metadata": {},
   "source": [
    "TF함수를 이용해 각 문서별로 사용된 단어수를 구해줍니다.  \n",
    "위와 같이 'am'은 1번, 'greate'은 2번 ... 사용된 것을  \n",
    "확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a150fe97-2927-4313-a66e-9702c7f8a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DF(text_tokenzied2, vocab):\n",
    "    text = []\n",
    "    for sentence in text_tokenzied2:\n",
    "        for word in list(set(sentence)):\n",
    "            text.append(word)\n",
    "    #print(text)\n",
    "    vocab_counter = Counter()\n",
    "    vocab_counter.update(text)\n",
    "\n",
    "    df = []\n",
    "    for word in vocab:\n",
    "        df.append(vocab_counter[word])\n",
    "    return df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a901d4b1-2bb6-4030-b456-167a99a3af6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(DF(text_tokenzied2, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06e304be-6890-4cb1-93d3-3f656a3c849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def IDF(df, n):\n",
    "    idf = []\n",
    "    for i in df:\n",
    "        idf.append(math.log((n)/(i+1))+1)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5cdb3d5-7071-4675-9621-3f91f6dd084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[0.5945348918918356, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17f82d28-e87c-4ce4-98ac-65526cd734fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[0.5945348918918356, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def TFIDF(tf, idf):\n",
    "    product = [x*y for x, y in zip(tf, idf)]\n",
    "    return product\n",
    "\n",
    "print(vocab)\n",
    "print(TFIDF(TF(vocab, count[1]), IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9822d8db-03f2-4f5f-8add-e9206f216e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[[0.5945348918918356, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.5945348918918356, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = []\n",
    "for c in count:\n",
    "    tfidf.append(TFIDF(TF(vocab, c), IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2))))\n",
    "\n",
    "print(vocab)\n",
    "print(tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
