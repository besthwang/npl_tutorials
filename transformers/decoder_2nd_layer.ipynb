{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6572748f-0e8e-4b37-8003-f045f22ec53b",
   "metadata": {},
   "source": [
    "## 디코더의 두번째 서브층: 인코더-디코더 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d9c6e-52df-4d9c-8628-b2283fb7514e",
   "metadata": {},
   "source": [
    "**참고문헌**  \n",
    "\\[1\\] [16-01 트랜스포머(Transformer)](https://wikidocs.net/31379)  \n",
    "\\[2\\] [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b38755b-d5fc-4348-a3c8-0dae6f69348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b9eac5-1b43-46e9-a918-121b598075fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "    \n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "#batch: number of batch\n",
    "#nbatches : nuber of batch run.\n",
    "#V : num of vacabulary\n",
    "def data_gen(V, batch, nbatches, seq_len=10):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, seq_len)))\n",
    "        #시작 토큰(Start Token)을 고정하는 역할\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)\n",
    "    \n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        #print('tgt_mask.shape', tgt_mask.shape)\n",
    "        #print('tgt_mask', tgt_mask)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    #x.shape = (batch_size, sequence_len)\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09980d8a-b238-41ae-bdfc-06e910cb8c72",
   "metadata": {},
   "source": [
    "![fig1](../images/decoder._look_a_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659be4d-b6d9-4ef4-9969-fa5a8ccbe7d3",
   "metadata": {},
   "source": [
    "디코더의 두번째 서브층에 대해서 이해해봅시다. 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는  \n",
    "점에서는 이전의 어텐션들(인코더와 디코더의 첫번째 서브층)과는 공통점이 있으나 이번에는 셀프 어텐션이 아님.  \n",
    "<br>\n",
    "셀프 어텐션은 Query, Key, Value가 같은 경우를 말하는데, 인코더-디코더 어텐션은 Query가 디코더인  \n",
    "행렬인 반면, Key와 Value는 인코더 행렬이기 때문입니다. 다시 한 번 각 서브층에서의 Q,K,V의 관계를 정리해봄.  \n",
    "\n",
    "|    layer             |                       |         \n",
    "|----------------------|------------------------|\n",
    "|인터더의 첫번째 서브층  | Query = **Key** = Value |\n",
    "|디코더의 첫번째 서브층  | Query = **Key** = Value |\n",
    "|디코더의 두번째 서브층  | Query : 디코더 행렬 /**Key = Value** : 인코더 행렬 |\n",
    "\n",
    "디코더의 두번째 서브층을 확대해 보면, 다음과 같이 인코더로부터 두 개의 화살표가 그려져 있음.  \n",
    "\n",
    "![fig_3](../images/decoder_2nd_layer.png)\n",
    "\n",
    "두 개의 화살표는 각각  key와 value를 의미하며 이는 인코더의 마지막 층에서 온 행렬로부터 얻음.  \n",
    "반면 Query는 더코더의 첫번째 서브층의 결과 행렬로부터 얻는다는 점이 다릅니다.  \n",
    "Query가 디코더 행렬 Key가 인코더 행렬일 때, 어텐셜 스코어 행렬을 추구하는 과정은 다음과 같습니다.\n",
    "\n",
    "![fig_4](../images/decoder_2nd_attention_score_matrix_final.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce1e641-11a8-4388-addf-0db63758bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector : row major \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    print(\"scores.shape : \", scores.shape)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    print(\"value.shape : \", value.shape)\n",
    "    print(\"p_attn.shape : \", p_attn.shape)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        print(\"query.shape \", query.shape)\n",
    "        print(\"key.shape \", key.shape)\n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2a59b-5cdc-42b0-b017-b897dc61681d",
   "metadata": {},
   "source": [
    "2nd layer의 attention layer에 들어 q,k,v값의 차원이 달라도  \n",
    "결과는 q의 차원으로 나옴.\n",
    "> 예  \n",
    "q = (1,9,512)  \n",
    "k = (1,10, 512)  \n",
    "v = (1,10, 512)  \n",
    "일 때  \n",
    "출력은 y =(1,9,512) 로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0291e5-ed9f-4b5a-b842-f39f4d8f0f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.shape :  torch.Size([1, 9, 512])\n",
      "m.shape :  torch.Size([1, 10, 512])\n",
      "query.shape  torch.Size([1, 8, 9, 64])\n",
      "key.shape  torch.Size([1, 8, 10, 64])\n",
      "scores.shape :  torch.Size([1, 8, 9, 10])\n",
      "value.shape :  torch.Size([1, 8, 10, 64])\n",
      "p_attn.shape :  torch.Size([1, 8, 9, 10])\n",
      "y.shape :  torch.Size([1, 9, 512])\n"
     ]
    }
   ],
   "source": [
    "V=10\n",
    "batch = 1\n",
    "d_model = 512\n",
    "h=8\n",
    "embedding = Embeddings(d_model, V)\n",
    "data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "#시작 토큰(Start Token)을 고정하는 역할\n",
    "data[:, 0] = 1\n",
    "src = Variable(data, requires_grad=False)\n",
    "tgt = Variable(data, requires_grad=False)\n",
    "batch = Batch(src, tgt, 0)\n",
    "m = embedding(batch.src)\n",
    "t = embedding(batch.trg)\n",
    "\n",
    "print(\"t.shape : \", t.shape)\n",
    "print(\"m.shape : \", m.shape)\n",
    "att = MultiHeadedAttention(h,d_model)\n",
    "\n",
    "y = att(t,m,m,batch.src_mask)\n",
    "\n",
    "print(\"y.shape : \", y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
