{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83b9b48-892f-4f45-b4df-b665158d1448",
   "metadata": {},
   "source": [
    "**참고문헌**  \n",
    "\\[1\\] [16-01 트랜스포머(Transformer)](https://wikidocs.net/31379)  \n",
    "\\[2\\] [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b4d0b-de91-450d-bd79-cf2aef8c2ae5",
   "metadata": {},
   "source": [
    "## 일반적인 trainning 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d4067-bae4-4151-8d7e-8f00876cbfc4",
   "metadata": {},
   "source": [
    "``` python\n",
    "model = Model(options)\n",
    "model.cuda()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = options.LR)\n",
    "\n",
    "for epoch in  range(numEpochs):\n",
    "    for sampleIndex, sample in enumerate(data_iterator): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(sample.input)\n",
    "        loss = compute_loass(output, sample.gt)\n",
    "        \n",
    "        #  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        save_weight()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c305aae-7a65-48fb-87e5-3d112a6ed134",
   "metadata": {},
   "source": [
    "**LabelSmoothing**이 `loss`을 계산함.\n",
    "- 정답데이터(ground truth)에 LabelSmoothing을 수행함.\n",
    "- ground truth는 아마도 인덱스 데이터 인듯\n",
    "- padding 데이터에는 masking\n",
    "- 아마도 예측 데이터와 gt로 KLDivLoss 계산\n",
    "- 출력은 아마도 loss.backward()하는데 사용되어야함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf9cd8-c3fa-4617-a18e-974e77f13c31",
   "metadata": {},
   "source": [
    "**NoamOpt**을 이해해봄\n",
    "- Optimizer를 래핑한 클래스임.\n",
    "- step() 메서드도 구현됨.\n",
    "- 학습률 계산을 계산하는 rate() 메서드가 구현됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a6c9f-2251-4013-8408-0d039f3f394a",
   "metadata": {},
   "source": [
    "back propagation하는 방식을 이해 해봄.\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aad9ae-0991-4ba7-81fb-05995b49962e",
   "metadata": {},
   "source": [
    "## Greedy Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730774b-4dd2-4a60-9e3a-cac9ad4e4eb0",
   "metadata": {},
   "source": [
    "``` python\n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss * norm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f916964-2b0b-4615-aa8a-1b842a4926cd",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Train the simple copy task.\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 30, 20), model, \n",
    "              SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "    model.eval()\n",
    "    print(run_epoch(data_gen(V, 30, 5), model, \n",
    "                    SimpleLossCompute(model.generator, criterion, None)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998dbb6-00da-48d5-a194-cef407665d88",
   "metadata": {},
   "source": [
    "``` python\n",
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc6d92-a16d-4783-bfa8-b7a1b344dd74",
   "metadata": {},
   "source": [
    "### Understading hwo to get loss and back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0df538-cfb8-4e9a-bbe5-452c2cf5bf35",
   "metadata": {},
   "source": [
    "1. **loss**의 계산은 `run_epoch`에서 수행됨  \n",
    "- `LabelSmoothing`과 `SimpleLossCompute` 클래스가 사용됨.\n",
    "- run_epoch는 먼저 `SimpleLossCompute` 클래스를 이용함.\n",
    "- 아래에 관련 코드가 있다.\n",
    "  \n",
    "---\n",
    "``` python\n",
    "loss = loss_compute(out, batch.trg_y, batch.ntokens) \n",
    "```\n",
    "---\n",
    "\n",
    "2. 이 부분을 먼저 보자.  \n",
    "예) paramers:   \n",
    "`out:` shape -> (8,9,11)  \n",
    "`batch.trg_y:` shape -> (8,9)  \n",
    "'batch.ntokens`: 8x9=72  (pad를 제외한 토큰의 수)  \n",
    "\n",
    "- 이함수는 SimpleLossCompute::\\_\\_call\\_\\_ 메서드를 호출함.  \n",
    "- `criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)`로 loss를 계산함.  \n",
    "  여기서 batch.trg_y를 out의 shape크기로 변현한후 LabelSmoothing으로 처리함.  \n",
    "        gt의 shape 변환 : (8,9) -> (8,9,11)  -> (72,11)   \n",
    "   **padding**는 LabelSmoothing이 적용되지 않음.  \n",
    "   transformer의 디코더 출력과 gt로  KLDivLoss을 구한 후 평균 loss를 구함.\n",
    "- `criterion`는 nn.Module를 상속 받아, forward()가 자동으로 호출됨.\n",
    "---\n",
    "\n",
    "3. `SimpleLossCompute'에서는 loss계산후에  backpropagation을 수행함.  \n",
    "\n",
    "``` python\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "\n",
    "```\n",
    "\n",
    "4. loss계산에 특이한 부분이 있음.\n",
    "   - loss계산할 때는 평균을 내고 backward()를 수행하고\n",
    "   - 리턴할 때는 다시 rescale을 함.\n",
    "     ``` python\n",
    "          return loss * norm\n",
    "     ```\n",
    "   - run_epoch 함수내에서 loss의 토탈평균을 다시 내고 있음.\n",
    "     ``` python\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        ...\n",
    "        return total_loss / total_tokens\n",
    "     ```\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d224d0-7c8c-42f1-9570-0dbc4e84ff2e",
   "metadata": {},
   "source": [
    "다음은 harvard transformer에 나온 Greedy Decoding 예제이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770516d0-e002-429b-a910-a6c2fdd390a5",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Train the simple copy task.\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 8, 1), model, \n",
    "              SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
