{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8e0af2-e284-4611-b055-458d9eda0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbccb0a-21d0-4806-930c-9133a3060b20",
   "metadata": {},
   "source": [
    "Implementation of harvard in pytorch  \n",
    "https://colab.research.google.com/drive/1-B8obSiAgAcq-VEJBVVzKIijySDRwm3k?usp=sharing#scrollTo=f15nl75sXT1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ee6b6d-4ed7-4862-937e-3beba408c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70c636-5b33-477f-9a41-5801304c1e6b",
   "metadata": {},
   "source": [
    "![Figure 2](../images/Multi-Head_Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4eadceb-75f4-42e8-9264-9af1bb7c3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    # mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "    # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = torch.matmul(query, key.transpose(-2, -1))  # (batch, heads, q_len, k_len)\n",
    "\n",
    "    print(\"matmul_qk.shape : \", matmul_qk.shape)\n",
    "    # 스케일링\n",
    "    # dk의 루트값으로 나눠준다.\n",
    "    depth = key.size()[-1]\n",
    "    logits = matmul_qk / torch.sqrt(torch.tensor(depth, dtype=torch.float32, device=query.device))\n",
    "\n",
    "    print(\"logits.shape : \", logits.shape)\n",
    "    \n",
    "    if mask is not None:\n",
    "        print(\"logits.shape : \", logits.shape) \n",
    "        logits = logits.masked_fill(mask == 0, float('-1e9'))\n",
    "\n",
    "    # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "    # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4fea2-c009-4b10-b65a-0bda6db79658",
   "metadata": {},
   "source": [
    "Linear의 shape이 $d\\_{model} \\times d\\_{model}$인 이유는 ?  \n",
    "multi-head 연산을 병렬로 수행하기 위해  $d\\_{model} \\times depth$ 형을 8번 붙여 놓은 것임\n",
    "``` python\n",
    "self.wq = nn.Linear(d_model, d_model)\n",
    "self.wk = nn.Linear(d_model, d_model)\n",
    "self.wv = nn.Linear(d_model, d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f9b92ff-580f-42be-9fc8-4031f54c57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)  # (batch_size, seq_len, num_heads, depth)\n",
    "        return x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1. Linear projections\n",
    "        q = self.wq(query)  # (batch_size, seq_len_q, d_model)\n",
    "        k = self.wk(key)    # (batch_size, seq_len_k, d_model)\n",
    "        v = self.wv(value)  # (batch_size, seq_len_v, d_model)\n",
    "        print(\"1th q.shape : \", q.shape)\n",
    "\n",
    "        # 2. Split into heads\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        print(\"2th q.shape : \", q.shape)\n",
    "        print(\"maks shape : \", mask.shape)\n",
    "        \n",
    "        # 3. Scaled dot-product attention\n",
    "        scaled_attention, attention_weight = scaled_dot_product_attention(q, k, v, mask)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        # 4. Concat heads\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        # 5. Final linear layer\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c44f7-11b7-4e9d-917c-4e74b18b0268",
   "metadata": {},
   "source": [
    "마스킹은 'Key'값을 기준으로 설정됨, Query랑은 상관 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ccd911-d421-45c3-b62a-3088982fcedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x,pad=0):\n",
    "    \"\"\"\n",
    "    x: (batch_size, seq_len)\n",
    "    return: mask of shape (batch_size, 1, 1, seq_len)\n",
    "    \"\"\"\n",
    "    mask = (x != pad).float()\n",
    "    return mask.unsqueeze(1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4ce53a-2b99-42d9-8345-e4d70df5de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c005f7e-77e8-4c64-93e7-ad4eb2a6c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = MultiHeadAttention(d_model, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d027e59-e514-436c-869a-a69916efa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embeddings(d_model,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660b05d1-f9b0-42d1-9f7d-a4670e30ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.random.randint(0, 11, size=(1, 10)))\n",
    "#x = embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8997be52-3e2b-49b5-8e28-8ff5771f2088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx input shape :  torch.Size([1, 10, 512])\n",
      "pad_mask shape :  torch.Size([1, 1, 1, 10])\n",
      "1th q.shape :  torch.Size([1, 10, 512])\n",
      "2th q.shape :  torch.Size([1, 8, 10, 64])\n",
      "maks shape :  torch.Size([1, 1, 1, 10])\n",
      "matmul_qk.shape :  torch.Size([1, 8, 10, 10])\n",
      "logits.shape :  torch.Size([1, 8, 10, 10])\n",
      "logits.shape :  torch.Size([1, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "xx = embedding(x)\n",
    "print(\"xx input shape : \", xx.shape)\n",
    "pad_mask = create_padding_mask(x)\n",
    "print(\"pad_mask shape : \", pad_mask.shape)\n",
    "y, weight = attn(xx,xx,xx,pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c0ea43-e514-4067-937e-1426491a6948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8c76601-e07f-401a-b3d9-739422cbf6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.0001e-01, 4.9999e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.0000e-01, 5.0000e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         4.9999e-01, 5.0001e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.5693e-06, 2.5693e-06, 0.0000e+00, 2.5693e-06, 0.0000e+00, 9.9999e-01,\n",
       "         0.0000e+00, 0.0000e+00, 2.5693e-06, 0.0000e+00]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
