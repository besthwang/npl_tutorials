{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0484215f-deb3-4cea-84d7-ef53e3f69dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b6ac0-59f7-4ab3-a950-caaec82dc382",
   "metadata": {},
   "source": [
    "[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)를 해석함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcef83f5-bce5-477c-83f3-cd9a0688334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c9e20-b7a1-4474-8667-82548d82af35",
   "metadata": {},
   "source": [
    "## Attention\n",
    "어텐션 함수는 쿼리(query)와 일련의 키-값(key-value)쌍을 출력으로 매핑하는 함수로   \n",
    "설명될 수 있으며, 여기서, 쿼리, 키, 값, 출력은 모두 벡터입니다. 출력은 값들의 가중합   \n",
    "(weighted sum)으로 계산되며, 각 값에 할당되는 가중치는 해당 키와 쿼리 간의 어텐션 함수  \n",
    "(compatibility function)를 통해 계산됨.  \n",
    "\n",
    "우리는 이러한 어텐션을 \"Scaled Dot-Production Attention)\"이라고 부릅니다.  \n",
    "입력은 차원이 $d_k$인 쿼리와 키, 그리고 차원이 $d_v$인 값을 포함함. 쿼리와 모든 키 간의  \n",
    "내적(dot product)을 계산한 후, 각 내적 값은 $\\sqrt(d_{k})$로 나누고, 소프트맥스(softmax)  \n",
    "함수를 적용하여 값에 대한 가중치를 얻음.    \n",
    "\n",
    "![fig1](../images/the-annotated-transformer_33_0.png)\n",
    "\n",
    "실제로는, 어텐션 함수는 여러 개의 쿼리에 대해 동시에 계산되며, 이들은 행렬 𝑄로 함께 묶어(packed)  \n",
    "처리됩니다. 키와 값도 각각 행렬 𝐾와 𝑉로 묶어 처리됩니다. 출력 행렬은 다음과 같이 계산됩니다:  \n",
    "\n",
    "$    \\text{attention}(Q,K,V)=softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688200ed-982f-4778-b2dd-4bad1048ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = 8(number of multi-head)\n",
    "#d_k = d_model/h\n",
    "#query.shape = [n_batch, h, seq_len, d_k]\n",
    "#key.shape   = [n_batch, h, seq_len, d_k]\n",
    "#value.shape = [n_batch, h, seq_len, d_k]\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04eca22-284a-4c7d-ba5e-ff7da12b947e",
   "metadata": {},
   "source": [
    "가장 널리 사용되는 두 가지 어텐션 함수는 가산 어텐션과 내적 어텐션입니다.  내적 어텐션은 우리가   \n",
    "제안한 알고리즘과 동일하지만, 차이점은 스케일링 계수 $\\frac{1}{\\sqrt{d_k}}$의 유무입니다.   \n",
    "\n",
    "가산 어텐션은 쿼리와 키의 적합도은 은닉층 하나로 구성된 피드포워드 신경망을 계산됨.  \n",
    "이 두 방식은 이론적인 계산 복잡도는 비슷하지만, 내적 어텐션은 고도로 최적화된 행렬 곱셉 코드를  \n",
    "사용할 수 있기 때문에, 실제로는 훨씬 빠르고 메모리 효율이 높습니다.  \n",
    "\n",
    "쿼리와 키의 차원 수 $d_k$가 작을 경우에는 두 메커니즘이 유사한 성능을 보이지만, $d_k$가 큰 경우에는  \n",
    "가산 어텐션이 스케일링이 적용되지 않은 내적 어텐션보다 더 나은 성능을 보입니다.  \n",
    "우리는 그 이유를 다음과 같이 추정합니다:  \n",
    "\n",
    "$d_k$가 클수록 쿼리 q와 키 k의 내적이 커지고, 이는 softmax 함수의 기울기 거의 0에 가까운 영역으로  \n",
    "들어가게 말들기 때문임. 예를 들어, q와 k의 각 성분이 평균이 0이고 분산 1인 독립적인 확률변수라고   \n",
    "가정하면, 그 내적 $ q\\cdot{k}=  \\sum_{i=0}^{n} q_ik_i $ 는 평균이 '0'이고 분산이 $d_k$ 임.  \n",
    "이 효과를 상쇄하기 위해, 우리는 내적에 $\\frac{1}{\\sqrt{d_k}}$을 곱함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db574d",
   "metadata": {},
   "source": [
    "\"$d_{model}$ 차원의 키(key), 값(value), 쿼리(query)를 사용해 단일 어텐션 함수를 수행하는 대신,\n",
    "쿼리, 키, 값을 각각 학습된 선형 변환(linear projection)을 통해 h번 투영하여, 각각 $d_k$, $d_k$, $d_v$ 차원으로 변환하는 것이 더 효과적이라는 것을 발견했습니다.<br>\n",
    "$Q=W^QQ, K=W^KK$,$V=W^VV$ <br>\n",
    "이렇게 투영된 쿼리, 키, 값 각각에 대해 병렬로 어텐션 연산을 수행하면, d_v 차원의 출력 벡터들이 생성됩니다. 이 출력 벡터들은 이어서 하나로 연결(concatenate)된 후, 다시 한 번 선형 변환을 통해 최종 출력 값이 생성됩니다. (이 과정은 그림 2에 나타나 있습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed4e71-04e7-4b66-8ef8-9d09c5377e24",
   "metadata": {},
   "source": [
    "![fig2](../images/the-annotated-transformer_38_0.png)  \n",
    "Figure 2: Multi-Head Attention consists of several attention layers running in parallel.\n",
    "\n",
    "**멀티-헤드 어텐션(Multi-head Attention)**은 모델이 서로 다른 위치에서  \n",
    "서로 다른 표현 하위 공간(representation subspace)의 정보를 동시에(attend jointly)  \n",
    "참조할 수 있도록 해줍니다. 단일 어텐션 헤드만 사용할 경우,   \n",
    "평균화(averaging)가 이러한 정보를 억제하는 경향이 있습니다.\n",
    "\n",
    "$ \\text{MultiHead}(Q,K,V)=\\text{Concat}(head_1,\\dots,head_h)W^O $\n",
    "\n",
    "여기서,  \n",
    "\n",
    "$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $  \n",
    "\n",
    "각 head에 대해 사용하는 프로젝션 행렬들은 다음과 같습니다:  \n",
    "\n",
    "- $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k}$\n",
    "- $W_i^K \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k}$\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{\\text{model}}\\times d_v}$\n",
    "- $W_i^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$\n",
    "\n",
    "이 논문에서는 **h=8** 개의 병렬 어텐션 레이어, 즉 8개의 헤드를 사용합니다. 각 헤드에 대해  \n",
    "**$d_k=d_v=d_{\\text{model}}/h=64 $** 설정됨.  \n",
    "각 헤드의 차원이 줄어들기 때문에, 전체 연산량은 전체 차원을 사용하는 단일 헤드 어텐션과   \n",
    "유사한 수준으로 유지됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1bed5c-f7e9-4ac0-a66b-08c3d3471e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        print(\"query.shape : \", query.shape)\n",
    "        print(\"key.shape   : \", key.shape)\n",
    "        print(\"value.shape : \", value.shape)\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        print(\"x.shape : \", x.shape)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        \n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70510b19-8a9e-46d1-9a3a-141b53833062",
   "metadata": {},
   "source": [
    "```  python\n",
    "query, key, value = [\n",
    "    lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "    for lin, x in zip(self.linears, (query, key, value))\n",
    "]\n",
    "```\n",
    "\n",
    "self.linears는 각 $W^Q, W^K, W^V, W^O$에 대한 행렬을 의미함.  \n",
    "$W^Q$는 $W^{d_{k}}$가 8개가 concat되어있는 행렬임.  \n",
    "이코드는 예를 들어 Q shape이 (n_batch, seq_len, d_model)의 크기 일때  \n",
    "$Q = W^Q \\times Q$ 이고 shape은 (n_batch, seq_len, d_model)이 되고  \n",
    "이걸 (n_batch,  h, seq_len, $d_k$)로 변경하는 코드임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa96a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9458529-779b-41fe-8f12-d5276d77e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch,d_model,h=1, 512, 8\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef1936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embeddings(d_model,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "417dc7fa-ecde-44f5-82ed-b89990e25a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx input shape :  torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randint(0, 11, size=(1, 10)))\n",
    "xx = embedding(x)\n",
    "print(\"xx input shape : \", xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a7cc15-6b7b-4593-b813-8cbcfd880ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape :  torch.Size([1, 8, 10, 64])\n",
      "key.shape   :  torch.Size([1, 8, 10, 64])\n",
      "value.shape :  torch.Size([1, 8, 10, 64])\n",
      "x.shape :  torch.Size([1, 8, 10, 64])\n",
      "out shape :  torch.Size([1, 10, 512])\n",
      "attn shape :  torch.Size([1, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attn = MultiHeadedAttention(h, d_model)\n",
    "out = attn(xx,xx,xx)\n",
    "print(\"out shape : \", out.shape)\n",
    "print(\"attn shape : \", attn.attn.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
