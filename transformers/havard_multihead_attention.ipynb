{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0484215f-deb3-4cea-84d7-ef53e3f69dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b6ac0-59f7-4ab3-a950-caaec82dc382",
   "metadata": {},
   "source": [
    "[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)ë¥¼ í•´ì„í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcef83f5-bce5-477c-83f3-cd9a0688334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c9e20-b7a1-4474-8667-82548d82af35",
   "metadata": {},
   "source": [
    "## Attention\n",
    "ì–´í…ì…˜ í•¨ìˆ˜ëŠ” ì¿¼ë¦¬(query)ì™€ ì¼ë ¨ì˜ í‚¤-ê°’(key-value)ìŒì„ ì¶œë ¥ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ë¡œ   \n",
    "ì„¤ëª…ë  ìˆ˜ ìˆìœ¼ë©°, ì—¬ê¸°ì„œ, ì¿¼ë¦¬, í‚¤, ê°’, ì¶œë ¥ì€ ëª¨ë‘ ë²¡í„°ì…ë‹ˆë‹¤. ì¶œë ¥ì€ ê°’ë“¤ì˜ ê°€ì¤‘í•©   \n",
    "(weighted sum)ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, ê° ê°’ì— í• ë‹¹ë˜ëŠ” ê°€ì¤‘ì¹˜ëŠ” í•´ë‹¹ í‚¤ì™€ ì¿¼ë¦¬ ê°„ì˜ ì–´í…ì…˜ í•¨ìˆ˜  \n",
    "(compatibility function)ë¥¼ í†µí•´ ê³„ì‚°ë¨.  \n",
    "\n",
    "ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì–´í…ì…˜ì„ \"Scaled Dot-Production Attention)\"ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.  \n",
    "ì…ë ¥ì€ ì°¨ì›ì´ $d_k$ì¸ ì¿¼ë¦¬ì™€ í‚¤, ê·¸ë¦¬ê³  ì°¨ì›ì´ $d_v$ì¸ ê°’ì„ í¬í•¨í•¨. ì¿¼ë¦¬ì™€ ëª¨ë“  í‚¤ ê°„ì˜  \n",
    "ë‚´ì (dot product)ì„ ê³„ì‚°í•œ í›„, ê° ë‚´ì  ê°’ì€ $\\sqrt(d_{k})$ë¡œ ë‚˜ëˆ„ê³ , ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax)  \n",
    "í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê°’ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì–»ìŒ.    \n",
    "\n",
    "![fig1](../images/the-annotated-transformer_33_0.png)\n",
    "\n",
    "ì‹¤ì œë¡œëŠ”, ì–´í…ì…˜ í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ì¿¼ë¦¬ì— ëŒ€í•´ ë™ì‹œì— ê³„ì‚°ë˜ë©°, ì´ë“¤ì€ í–‰ë ¬ ğ‘„ë¡œ í•¨ê»˜ ë¬¶ì–´(packed)  \n",
    "ì²˜ë¦¬ë©ë‹ˆë‹¤. í‚¤ì™€ ê°’ë„ ê°ê° í–‰ë ¬ ğ¾ì™€ ğ‘‰ë¡œ ë¬¶ì–´ ì²˜ë¦¬ë©ë‹ˆë‹¤. ì¶œë ¥ í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:  \n",
    "\n",
    "$    \\text{attention}(Q,K,V)=softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688200ed-982f-4778-b2dd-4bad1048ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = 8(number of multi-head)\n",
    "#d_k = d_model/h\n",
    "#query.shape = [n_batch, h, seq_len, d_k]\n",
    "#key.shape   = [n_batch, h, seq_len, d_k]\n",
    "#value.shape = [n_batch, h, seq_len, d_k]\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04eca22-284a-4c7d-ba5e-ff7da12b947e",
   "metadata": {},
   "source": [
    "ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ì–´í…ì…˜ í•¨ìˆ˜ëŠ” ê°€ì‚° ì–´í…ì…˜ê³¼ ë‚´ì  ì–´í…ì…˜ì…ë‹ˆë‹¤.  ë‚´ì  ì–´í…ì…˜ì€ ìš°ë¦¬ê°€   \n",
    "ì œì•ˆí•œ ì•Œê³ ë¦¬ì¦˜ê³¼ ë™ì¼í•˜ì§€ë§Œ, ì°¨ì´ì ì€ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ $\\frac{1}{\\sqrt{d_k}}$ì˜ ìœ ë¬´ì…ë‹ˆë‹¤.   \n",
    "\n",
    "ê°€ì‚° ì–´í…ì…˜ì€ ì¿¼ë¦¬ì™€ í‚¤ì˜ ì í•©ë„ì€ ì€ë‹‰ì¸µ í•˜ë‚˜ë¡œ êµ¬ì„±ëœ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì„ ê³„ì‚°ë¨.  \n",
    "ì´ ë‘ ë°©ì‹ì€ ì´ë¡ ì ì¸ ê³„ì‚° ë³µì¡ë„ëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ, ë‚´ì  ì–´í…ì…˜ì€ ê³ ë„ë¡œ ìµœì í™”ëœ í–‰ë ¬ ê³±ì…‰ ì½”ë“œë¥¼  \n",
    "ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì‹¤ì œë¡œëŠ” í›¨ì”¬ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤.  \n",
    "\n",
    "ì¿¼ë¦¬ì™€ í‚¤ì˜ ì°¨ì› ìˆ˜ $d_k$ê°€ ì‘ì„ ê²½ìš°ì—ëŠ” ë‘ ë©”ì»¤ë‹ˆì¦˜ì´ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, $d_k$ê°€ í° ê²½ìš°ì—ëŠ”  \n",
    "ê°€ì‚° ì–´í…ì…˜ì´ ìŠ¤ì¼€ì¼ë§ì´ ì ìš©ë˜ì§€ ì•Šì€ ë‚´ì  ì–´í…ì…˜ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.  \n",
    "ìš°ë¦¬ëŠ” ê·¸ ì´ìœ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ì •í•©ë‹ˆë‹¤:  \n",
    "\n",
    "$d_k$ê°€ í´ìˆ˜ë¡ ì¿¼ë¦¬ qì™€ í‚¤ kì˜ ë‚´ì ì´ ì»¤ì§€ê³ , ì´ëŠ” softmax í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° ê±°ì˜ 0ì— ê°€ê¹Œìš´ ì˜ì—­ìœ¼ë¡œ  \n",
    "ë“¤ì–´ê°€ê²Œ ë§ë“¤ê¸° ë•Œë¬¸ì„. ì˜ˆë¥¼ ë“¤ì–´, qì™€ kì˜ ê° ì„±ë¶„ì´ í‰ê· ì´ 0ì´ê³  ë¶„ì‚° 1ì¸ ë…ë¦½ì ì¸ í™•ë¥ ë³€ìˆ˜ë¼ê³    \n",
    "ê°€ì •í•˜ë©´, ê·¸ ë‚´ì  $ q\\cdot{k}=  \\sum_{i=0}^{n} q_ik_i $ ëŠ” í‰ê· ì´ '0'ì´ê³  ë¶„ì‚°ì´ $d_k$ ì„.  \n",
    "ì´ íš¨ê³¼ë¥¼ ìƒì‡„í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë‚´ì ì— $\\frac{1}{\\sqrt{d_k}}$ì„ ê³±í•¨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db574d",
   "metadata": {},
   "source": [
    "\"$d_{model}$ ì°¨ì›ì˜ í‚¤(key), ê°’(value), ì¿¼ë¦¬(query)ë¥¼ ì‚¬ìš©í•´ ë‹¨ì¼ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹ ,\n",
    "ì¿¼ë¦¬, í‚¤, ê°’ì„ ê°ê° í•™ìŠµëœ ì„ í˜• ë³€í™˜(linear projection)ì„ í†µí•´ hë²ˆ íˆ¬ì˜í•˜ì—¬, ê°ê° $d_k$, $d_k$, $d_v$ ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.<br>\n",
    "$Q=W^QQ, K=W^KK$,$V=W^VV$ <br>\n",
    "ì´ë ‡ê²Œ íˆ¬ì˜ëœ ì¿¼ë¦¬, í‚¤, ê°’ ê°ê°ì— ëŒ€í•´ ë³‘ë ¬ë¡œ ì–´í…ì…˜ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©´, d_v ì°¨ì›ì˜ ì¶œë ¥ ë²¡í„°ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤. ì´ ì¶œë ¥ ë²¡í„°ë“¤ì€ ì´ì–´ì„œ í•˜ë‚˜ë¡œ ì—°ê²°(concatenate)ëœ í›„, ë‹¤ì‹œ í•œ ë²ˆ ì„ í˜• ë³€í™˜ì„ í†µí•´ ìµœì¢… ì¶œë ¥ ê°’ì´ ìƒì„±ë©ë‹ˆë‹¤. (ì´ ê³¼ì •ì€ ê·¸ë¦¼ 2ì— ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed4e71-04e7-4b66-8ef8-9d09c5377e24",
   "metadata": {},
   "source": [
    "![fig2](../images/the-annotated-transformer_38_0.png)  \n",
    "Figure 2: Multi-Head Attention consists of several attention layers running in parallel.\n",
    "\n",
    "**ë©€í‹°-í—¤ë“œ ì–´í…ì…˜(Multi-head Attention)**ì€ ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œ  \n",
    "ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ í•˜ìœ„ ê³µê°„(representation subspace)ì˜ ì •ë³´ë¥¼ ë™ì‹œì—(attend jointly)  \n",
    "ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤. ë‹¨ì¼ ì–´í…ì…˜ í—¤ë“œë§Œ ì‚¬ìš©í•  ê²½ìš°,   \n",
    "í‰ê· í™”(averaging)ê°€ ì´ëŸ¬í•œ ì •ë³´ë¥¼ ì–µì œí•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$ \\text{MultiHead}(Q,K,V)=\\text{Concat}(head_1,\\dots,head_h)W^O $\n",
    "\n",
    "ì—¬ê¸°ì„œ,  \n",
    "\n",
    "$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $  \n",
    "\n",
    "ê° headì— ëŒ€í•´ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì ì…˜ í–‰ë ¬ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:  \n",
    "\n",
    "- $W_i^Q \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k}$\n",
    "- $W_i^K \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k}$\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{\\text{model}}\\times d_v}$\n",
    "- $W_i^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$\n",
    "\n",
    "ì´ ë…¼ë¬¸ì—ì„œëŠ” **h=8** ê°œì˜ ë³‘ë ¬ ì–´í…ì…˜ ë ˆì´ì–´, ì¦‰ 8ê°œì˜ í—¤ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° í—¤ë“œì— ëŒ€í•´  \n",
    "**$d_k=d_v=d_{\\text{model}}/h=64 $** ì„¤ì •ë¨.  \n",
    "ê° í—¤ë“œì˜ ì°¨ì›ì´ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì—, ì „ì²´ ì—°ì‚°ëŸ‰ì€ ì „ì²´ ì°¨ì›ì„ ì‚¬ìš©í•˜ëŠ” ë‹¨ì¼ í—¤ë“œ ì–´í…ì…˜ê³¼   \n",
    "ìœ ì‚¬í•œ ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€ë¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1bed5c-f7e9-4ac0-a66b-08c3d3471e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        print(\"query.shape : \", query.shape)\n",
    "        print(\"key.shape   : \", key.shape)\n",
    "        print(\"value.shape : \", value.shape)\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        print(\"x.shape : \", x.shape)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        \n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70510b19-8a9e-46d1-9a3a-141b53833062",
   "metadata": {},
   "source": [
    "```  python\n",
    "query, key, value = [\n",
    "    lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "    for lin, x in zip(self.linears, (query, key, value))\n",
    "]\n",
    "```\n",
    "\n",
    "self.linearsëŠ” ê° $W^Q, W^K, W^V, W^O$ì— ëŒ€í•œ í–‰ë ¬ì„ ì˜ë¯¸í•¨.  \n",
    "$W^Q$ëŠ” $W^{d_{k}}$ê°€ 8ê°œê°€ concatë˜ì–´ìˆëŠ” í–‰ë ¬ì„.  \n",
    "ì´ì½”ë“œëŠ” ì˜ˆë¥¼ ë“¤ì–´ Q shapeì´ (n_batch, seq_len, d_model)ì˜ í¬ê¸° ì¼ë•Œ  \n",
    "$Q = W^Q \\times Q$ ì´ê³  shapeì€ (n_batch, seq_len, d_model)ì´ ë˜ê³   \n",
    "ì´ê±¸ (n_batch,  h, seq_len, $d_k$)ë¡œ ë³€ê²½í•˜ëŠ” ì½”ë“œì„.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa96a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9458529-779b-41fe-8f12-d5276d77e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch,d_model,h=1, 512, 8\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef1936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embeddings(d_model,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "417dc7fa-ecde-44f5-82ed-b89990e25a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx input shape :  torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randint(0, 11, size=(1, 10)))\n",
    "xx = embedding(x)\n",
    "print(\"xx input shape : \", xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a7cc15-6b7b-4593-b813-8cbcfd880ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape :  torch.Size([1, 8, 10, 64])\n",
      "key.shape   :  torch.Size([1, 8, 10, 64])\n",
      "value.shape :  torch.Size([1, 8, 10, 64])\n",
      "x.shape :  torch.Size([1, 8, 10, 64])\n",
      "out shape :  torch.Size([1, 10, 512])\n",
      "attn shape :  torch.Size([1, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "attn = MultiHeadedAttention(h, d_model)\n",
    "out = attn(xx,xx,xx)\n",
    "print(\"out shape : \", out.shape)\n",
    "print(\"attn shape : \", attn.attn.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
