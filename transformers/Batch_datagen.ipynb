{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb6b353-212c-4e10-bb42-e7b0d517fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250842e-b338-4c33-b388-5a552df038a4",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd22b49-6a82-4adc-b025-d88c42bf571e",
   "metadata": {},
   "source": [
    "모델을 학습하기 위해 encoder-decoder 데이터를 준비하는 클래스임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291a405-eae6-4e65-853f-e5b1a7c49df0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Batch:**\n",
    "\n",
    "``` python\n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "           ...\n",
    "```\n",
    "`src_mask`:  \n",
    "pad가 '0'일 때 mask값은 0임.  \n",
    "encoder의 attention에서는 mask == 0 일 때,  \n",
    "scores = scores.masked_fill(mask == 0, -1e9)  \n",
    "\n",
    "`trg :`\n",
    "여기서 trg는 일반적으로 목표 문장(타겟 시퀀스) 을 의미하고,  \n",
    "기계번역이나 시퀀스 생성 모델에서는 다음과 같은 두 가지 역할로 나뉘어 사용됩니다:  \n",
    "\n",
    "`decoder input`: 이전 단어까지의 시퀀스를 입력으로 사용  \n",
    "`target output`: 실제 정답 시퀀스 (예측 대상)  \n",
    "\n",
    "`trg[:, :-1]의 의미`  \n",
    "trg[:, :-1]은 타겟 시퀀스의 마지막 토큰을 잘라낸 것입니다.  \n",
    "예를 들어 trg가 다음과 같다면:  \n",
    "\n",
    "trg = [[BOS, A, B, C, EOS]]  \n",
    "여기서:  \n",
    "\n",
    "BOS: Begin of Sentence  \n",
    "EOS: End of Sentence  \n",
    "trg[:, :-1]은 다음과 같은 결과를 만듭니다:  \n",
    "\n",
    "`[[BOS, A, B, C]]`  \n",
    "\n",
    "`디코더의 입력:`  \n",
    "왜 이렇게 하나요?  \n",
    "시퀀스 투 시퀀스 모델(예: Transformer, LSTM 기반 NMT)은 일반적으로 디코더에 이전 단어들을 입력으로 주고,  \n",
    "다음 단어를 예측합니다.  \n",
    "\n",
    "입력 시퀀스: [BOS, A, B, C] → 디코더에 들어감  \n",
    "예측 대상: [A, B, C, EOS] → 모델이 맞혀야 할 정답  \n",
    "\n",
    "  \n",
    "\n",
    "|디코더 입력 (trg[:, :-1])|\t예측 대상 (trg[:, 1:]) | \n",
    "|---------------------------|----------------------|\n",
    "|[BOS, A, B, C]\t| [A, B, C, EOS]                  | \n",
    "\n",
    "이렇게 나누어야 디코더가 한 단계씩 다음 토큰을 예측할 수 있어요.  \n",
    "\n",
    "`요약:`  \n",
    "self.trg = trg[:, :-1]는 디코더 입력을 만들기 위한 코드입니다.  \n",
    "타겟 시퀀스에서 마지막 토큰(EOS 등)을 제거해 이전 단어까지만 남기는 역할을 합니다.  \n",
    "이렇게 전처리해야 디코더가 시퀀스를 한 단계씩 예측할 수 있게 됩니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df691e-e3fa-42a9-8a10-7195c160525d",
   "metadata": {},
   "source": [
    "\n",
    "`self.trg_mask 분석해봄`:\n",
    "---\n",
    "\n",
    "``` python\n",
    "self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "```\n",
    "\n",
    "디코더의 self.attention에서 사용할 마스크(mask)를 생성할 코드임.  \n",
    "  \n",
    "**목적 요약**: \n",
    "- Padding 토큰 무시하기  \n",
    "→ trg 안의 pad 토큰은 무시되어야 함 (학습에 영향을 주면 안 됨) \n",
    "\n",
    "- 미래 단어 가리기 (Look-ahead masking)  \n",
    "→ 현재 시점보다 미래 단어는 보지 못하게 막기 (언어 모델처럼 auto-regressive하게 학습하기 위함)\n",
    "\n",
    "\n",
    "``` python\n",
    "def make_std_mask(tgt, pad):\n",
    "    # padding이 아닌 부분은 True\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)  # shape: [batch, 1, seq_len]\n",
    "\n",
    "    # look-ahead mask 추가 (미래 단어 보지 못하게)\n",
    "    tgt_mask = tgt_mask & Variable(\n",
    "        subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))  # shape: [1, seq_len, seq_len]\n",
    "\n",
    "    return tgt_mask\n",
    "```\n",
    "\n",
    "`subsequent_mask(tgt.size(-1))`  \n",
    "이 함수는 하삼각 행렬(triangular matrix) 형태의 마스크를 만듭니다.  \n",
    "시점 t에서는 t보다 이후 시점 단어를 보지 못하게끔 만듭니다.  \n",
    "예: 길이 5인 문장이라면  \n",
    "```\n",
    "[[1, 0, 0, 0, 0],\n",
    " [1, 1, 0, 0, 0],\n",
    " [1, 1, 1, 0, 0],\n",
    " [1, 1, 1, 1, 0],\n",
    " [1, 1, 1, 1, 1]]\n",
    "```\n",
    "\n",
    "``` python\n",
    " tgt_mask = tgt_mask & Variable(\n",
    "        subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))  # shape: [1, seq_len, seq_len]\n",
    "```\n",
    "\n",
    "이렇게 하면 pad_mask == 0 부분을 값도 false로 되어 soft_mask값에 반영되지 않음.  \n",
    "여기서, tgt_mask는 broadcasting 되어 연산됨.\n",
    "예를 들면:  \n",
    "``` python\n",
    "m = subsequent_mask(3)\n",
    "t = torch.tensor([[[False,True, True]]])\n",
    "print(t&m)\n",
    "```\n",
    "결과는 :\n",
    "tensor([[[False, False, False],\n",
    "         [False,  True, False],\n",
    "         [False,  True,  True]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ccea2bd-58f2-46d6-bf3a-5108ab7f0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "#batch: number of batch\n",
    "#nbatches : nuber of batch run.\n",
    "#V : num of vacabulary\n",
    "def data_gen(V, batch, nbatches, seq_len=10):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, seq_len)))\n",
    "        #시작 토큰(Start Token)을 고정하는 역할\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)\n",
    "    \n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        #print('tgt_mask.shape', tgt_mask.shape)\n",
    "        #print('tgt_mask', tgt_mask)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0857d3-07a7-4a83-8f54-990f754f5b60",
   "metadata": {},
   "source": [
    "### Batch 클래스 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2929e0-1564-40f6-8b56-fa9b773e5600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_mask.shape torch.Size([30, 1, 9])\n",
      "tgt_mask tensor([[[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "nV = 10 #dim of vocabulary\n",
    "seq_len = 10\n",
    "batch = 30\n",
    "nbatches = 20\n",
    "data = torch.from_numpy(np.random.randint(1, nV, size=(batch, seq_len)))\n",
    "#시작 토큰(Start Token)을 고정하는 역할\n",
    "data[:, 0] = 1\n",
    "src = Variable(data, requires_grad=False)\n",
    "tgt = Variable(data, requires_grad=False)\n",
    "inputs =  Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918c87d-5172-4d8b-878d-2c9252f36736",
   "metadata": {},
   "source": [
    "`data_gen` 함수는 V 크기의 단어로 이루어진, batch 크기의 입출력 데이터를  \n",
    "`n_batches` 횟수만큼 발생시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca8156c-2188-4185-a97f-05ddb5bf86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iters = data_gen(nV,batch,nbatches,seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
