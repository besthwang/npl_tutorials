{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f579fa1-1f41-470a-917d-17838b6bf21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from konlpy.tag import Okt\n",
    "import spacy\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf08ec3-07e3-4d85-ab19-9f7d6f9ae458",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4945eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus = [\n",
    "    {\"en\": \"Hello.\", \"ko\": \"안녕하세요.\"},\n",
    "    {\"en\": \"How are you?\", \"ko\": \"잘 지내세요?\"},\n",
    "    {\"en\": \"I am a student.\", \"ko\": \"저는 학생입니다.\"},\n",
    "    {\"en\": \"What is your name?\", \"ko\": \"당신의 이름은 무엇인가요?\"},\n",
    "    {\"en\": \"My name is John.\", \"ko\": \"제 이름은 존입니다.\"},\n",
    "    {\"en\": \"Nice to meet you.\", \"ko\": \"만나서 반갑습니다.\"},\n",
    "    {\"en\": \"Good morning!\", \"ko\": \"좋은 아침이에요!\"},\n",
    "    {\"en\": \"Good night.\", \"ko\": \"안녕히 주무세요.\"},\n",
    "    {\"en\": \"Where are you going?\", \"ko\": \"어디 가세요?\"},\n",
    "    {\"en\": \"I’m going home.\", \"ko\": \"집에 가는 중이에요.\"},\n",
    "    {\"en\": \"See you later.\", \"ko\": \"나중에 봐요.\"},\n",
    "    {\"en\": \"Thank you very much.\", \"ko\": \"정말 감사합니다.\"},\n",
    "    {\"en\": \"You're welcome.\", \"ko\": \"천만에요.\"},\n",
    "    {\"en\": \"Excuse me.\", \"ko\": \"실례합니다.\"},\n",
    "    {\"en\": \"I’m sorry.\", \"ko\": \"죄송합니다.\"},\n",
    "    {\"en\": \"No problem.\", \"ko\": \"괜찮아요.\"},\n",
    "    {\"en\": \"Do you speak English?\", \"ko\": \"영어 할 줄 아세요?\"},\n",
    "    {\"en\": \"I speak a little Korean.\", \"ko\": \"한국어 조금 할 줄 알아요.\"},\n",
    "    {\"en\": \"How much is this?\", \"ko\": \"이거 얼마예요?\"},\n",
    "    {\"en\": \"Where is the restroom?\", \"ko\": \"화장실 어디예요?\"},\n",
    "    {\"en\": \"Can you help me?\", \"ko\": \"도와주실 수 있나요?\"},\n",
    "    {\"en\": \"I don’t understand.\", \"ko\": \"이해하지 못했어요.\"},\n",
    "    {\"en\": \"Please speak slowly.\", \"ko\": \"천천히 말씀해 주세요.\"},\n",
    "    {\"en\": \"I’m hungry.\", \"ko\": \"배고파요.\"},\n",
    "    {\"en\": \"I’m tired.\", \"ko\": \"피곤해요.\"},\n",
    "    {\"en\": \"What time is it?\", \"ko\": \"지금 몇 시예요?\"},\n",
    "    {\"en\": \"Today is Monday.\", \"ko\": \"오늘은 월요일이에요.\"},\n",
    "    {\"en\": \"It’s raining.\", \"ko\": \"비가 오고 있어요.\"},\n",
    "    {\"en\": \"I like music.\", \"ko\": \"저는 음악을 좋아해요.\"},\n",
    "    {\"en\": \"Let’s go together.\", \"ko\": \"같이 가요.\"},\n",
    "    {\"en\": \"Have a nice day!\", \"ko\": \"좋은 하루 되세요!\"},\n",
    "    {\"en\": \"Goodbye!\", \"ko\": \"안녕히 가세요!\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8d3ae8-f2d2-4955-a5f7-5b67dfe501c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class En2KoDataset(Dataset):\n",
    "    def __init__(self, corpus, okt, nlp, random=False, special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]):\n",
    "        \n",
    "        self.random = random\n",
    "        \n",
    "        self.src_sentences = []\n",
    "        self.tgt_sentences = []\n",
    "        \n",
    "        self.en_tokens = set()\n",
    "        self.ko_tokens = set()\n",
    "        for line in parallel_corpus:\n",
    "            self.src_sentences.append(line['en'])\n",
    "            self.tgt_sentences.append(line['ko'])\n",
    "            self.ko_tokens.update(okt.morphs(line['ko']))\n",
    "            en_list = [ token.text for token in nlp(line['en'])]\n",
    "            self.en_tokens.update(en_list)\n",
    "    \n",
    "        self.special_tokens = special_tokens\n",
    "    \n",
    "        self.en_vocab = special_tokens + sorted(list(self.en_tokens))\n",
    "        self.en_token2idx = {token: idx for idx, token in enumerate(self.en_vocab)}\n",
    "        self.en_idx2token = {idx: token for token, idx in self.en_token2idx.items()}\n",
    "        \n",
    "        self.ko_vocab = special_tokens + sorted(list(self.ko_tokens))\n",
    "        self.ko_token2idx = {token: idx for idx, token in enumerate(self.ko_vocab)}\n",
    "        self.ko_idx2token = {idx: token for token, idx in self.ko_token2idx.items()}\n",
    "\n",
    "        \n",
    "\n",
    "        self.srcs = []\n",
    "        self.trgs = []\n",
    "        #self.trgs_y = []\n",
    "        \n",
    "        for line in parallel_corpus:\n",
    "            trg = self.en_encode(line['en'], self.en_token2idx)\n",
    "            self.trgs.append(trg)\n",
    "            #self.trgs_y.append(trg[1:])\n",
    "            src = self.ko_encode(line['ko'], self.ko_token2idx) \n",
    "            self.srcs.append(src)\n",
    "\n",
    "        self.n_sentences = len(self.src_sentences) \n",
    "        print(self.n_sentences)\n",
    "            \n",
    "    def ko_encode(self, sentence, token2idx):\n",
    "        tokens = okt.morphs(sentence)\n",
    "        ids = [token2idx.get(tok, token2idx[\"<unk>\"]) for tok in tokens]\n",
    "        out = None\n",
    "        out = [token2idx[\"<sos>\"]] + ids + [token2idx[\"<eos>\"]]   \n",
    "        return out\n",
    "\n",
    "\n",
    "    def en_encode(self, sentence, token2idx):\n",
    "        tokens = [ token.text for token in nlp(sentence)]\n",
    "        ids = [token2idx.get(tok, token2idx[\"<unk>\"]) for tok in tokens]\n",
    "        out = None\n",
    "        out = [token2idx[\"<sos>\"]] + ids + [token2idx[\"<eos>\"]]   \n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(f\"index: {index}\")\n",
    "        # if self.random:\n",
    "        #     t = int(time.time() * 1000000)\n",
    "        #     np.random.seed(((t & 0xff000000) >> 24) +\n",
    "        #                    ((t & 0x00ff0000) >> 8) +\n",
    "        #                    ((t & 0x0000ff00) << 8) +\n",
    "        #                    ((t & 0x000000ff) << 24))\n",
    "        #     index = np.random.randint(self.n_sentences)\n",
    "        # else:\n",
    "        #     index = index % self.n_sentences\n",
    "        #     pass\n",
    "        #index = index % self.__len__()\n",
    "        \n",
    "        sample = self.srcs[index], self.trgs[index]\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529af098-dd0c-4ea1-b0d3-ba6af2b1ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "dataset = En2KoDataset(parallel_corpus, okt, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c766954-e445-42ce-b52a-d0f5870f9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "    \n",
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        #print('tgt_mask.shape', tgt_mask.shape)\n",
    "        #print('tgt_mask', tgt_mask)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "        \n",
    "def custom_collate_fn(raw_data):\n",
    "    srcs, trgs = zip(*raw_data)  # unpack batch list of triples\n",
    "\n",
    "    srcs = [torch.tensor(seq) for seq in srcs]\n",
    "    trgs = [torch.tensor(seq) for seq in trgs]\n",
    "\n",
    "    srcs_padded = pad_sequence(srcs, batch_first=True, padding_value=0)\n",
    "    trgs_padded = pad_sequence(trgs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return Batch(srcs_padded, trgs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35674517-38bf-4e0e-8ab4-f76f2d72cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68fdaa64-710b-423b-9c48-d0fafc70cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: torch.Size([4, 8])\n",
      "Target: torch.Size([4, 7])\n",
      "Source: torch.Size([4, 8])\n",
      "Target: torch.Size([4, 6])\n",
      "Source: torch.Size([4, 6])\n",
      "Target: torch.Size([4, 6])\n",
      "Source: torch.Size([4, 8])\n",
      "Target: torch.Size([4, 6])\n",
      "Source: torch.Size([4, 7])\n",
      "Target: torch.Size([4, 6])\n",
      "Source: torch.Size([4, 7])\n",
      "Target: torch.Size([4, 6])\n",
      "Source: torch.Size([4, 9])\n",
      "Target: torch.Size([4, 6])\n",
      "Source: torch.Size([4, 7])\n",
      "Target: torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    #src_batch, trg_batch  = batch\n",
    "    print(\"Source:\", batch.src.shape)     # (B, T_src)\n",
    "    print(\"Target:\", batch.trg.shape)     # (B, T_trg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
