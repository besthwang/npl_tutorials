## attention 관련 문서 정리
- [16-01 트랜스포머(Transformer)](attention.ipynb) : <br>
  [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379)에 있는 내용을 정리함.
  attention을 이해하기 좋은 자료임.
- [harvard attention 구현 및 attention 논문](./havard_multihead_attention.ipynb) :
  논문설명과 harvard 코드 구현, 사용법이 설명됨
- [MultiHeadAttention](MultiHeadAttention.ipynb) :  `딥 러닝을 이용한 자연어 처리 입문`에 <br>
있는 멀티헤드 attention을 pytorch 버전으로 변경하고, harvard 버전의 data로 테스트 할 수 있음.

## Position Encoding
- [positional_encodingII.ipynb](./positional_encodingII.ipynb) : [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379)에 있는 내용을 정리함

## Feed-Forward Networks
- [Position-wise-FFNN.ipynb](./Position-wise-FFNN.ipynb) [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379)에 있는 내용을 정리함.

## Masking
- [Understanding_Masking_in_PyTorch_for_Attention_Mechanisms.ipynb](Understanding_Masking_in_PyTorch_for_Attention_Mechanisms.ipynb) :
  [Understanding Masking in PyTorch for Attention Mechanisms](https://medium.com/@swarms/understanding-masking-in-pytorch-for-attention-mechanisms-e725059fd49f) 를 참고함.
- [decoder_selfattention_look_a_head_mask.ipynb](./decoder_selfattention_look_a_head_mask.ipynb) : [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379)에 있는 내용을 정리함