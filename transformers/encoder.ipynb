{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af93b8e1-2c27-443a-a0dd-3a75b91f3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "#seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bba61f-4d2d-4816-8ea9-d061f52d41ae",
   "metadata": {},
   "source": [
    "참고문헌  \n",
    "\\[1\\] [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims)  \n",
    "\\[2\\] [new The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)  \n",
    "\\[3\\] [git for new The Annotated Transforme](https://github.com/harvardnlp/annotated-transformer.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e71c0d6-73b2-44d6-add9-e2dc67f159cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x,pad=0):\n",
    "    \"\"\"\n",
    "    x: (batch_size, seq_len)\n",
    "    return: mask of shape (batch_size, 1, 1, seq_len)\n",
    "    \"\"\"\n",
    "    mask = (x != pad).float()\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "        \n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb7889-807b-4148-88b6-2a21e32620f7",
   "metadata": {},
   "source": [
    "### Fig1. Left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f7c1d-c94e-4ed0-8c34-3c1cc505b374",
   "metadata": {},
   "source": [
    "![fig_1](../images/add_norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56e4cf-9305-425d-8957-ac11530a3dcb",
   "metadata": {},
   "source": [
    "We employ a residual connection ([Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)) around each of the two sub-layers,   \n",
    "followed by layer normalization ([Layer Normalization](https://arxiv.org/abs/1607.06450))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bd4d1f-5277-4740-86cd-1bbf8a750a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9597c-bca5-41e8-b809-fe55e04ab925",
   "metadata": {},
   "source": [
    "여기서, self.a_2 * x 연산은  원소별 곱셉을 의미함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1a0e5c-932f-4f78-b285-e41429f04559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5f9d4-826f-494f-acf2-12a734db063c",
   "metadata": {},
   "source": [
    "``` python\n",
    " x + self.dropout(sublayer(self.norm(x)))\n",
    "```\n",
    "\n",
    "이 방식은 Pre-Layer Normalization 또는 Pre-Norm 방식임.  \n",
    "이 방식은 Transformer의 초기 버전과는 반대되는 구조이며, 여러 현대 Transformer 변형  \n",
    "모델에서 선호됨. (gpt에게 물어봄)\n",
    "\n",
    "왜 Pre-Norm을 사용하나?  \n",
    "1. 학습 안정성 향상  \n",
    "깊은 Transformer 구조에서는 Post-Norm(LayerNorm(x + Sublayer(x))) 구조가  \n",
    "그래디언트 소실 문제를 야기할 수 있습니다.  \n",
    "반면, Pre-Norm은 정규화를 먼저 적용해 잔차 경로를 더 잘 유지할 수 있어서  \n",
    "깊은 네트워크에서도 학습이 안정적으로 진행됩니다.\n",
    "\n",
    "2. 그래디언트 흐름 개선  \n",
    "Pre-Norm은 x 경로가 LayerNorm 앞에 있기 때문에, 그래디언트가 직접 x로 잘 전달됩니다.  \n",
    "즉, 그래디언트가 서브레이어(sublayer)에서 멈추지 않고 잘 흘러서, 깊은 모델 학습에 유리합니다.  \n",
    "\n",
    "3. 최근 모델들이 채택  \n",
    "GPT-2, T5, BERT-large 등 다양한 Transformer 기반 모델들이 이 구조 또는 변형 구조를 사용.  \n",
    "특히 T5는 모든 블록에 Pre-Norm을 적용하여 안정성과 효율성을 확보합니다.\n",
    "\n",
    "\n",
    "4. [Understanding the Difficulty of Training Transformers](https://arxiv.org/abs/2004.08249) 논문에서 Pre-Norm 구조의 이점이 설명됨. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed52d84-5af1-404c-8cd1-1dee36ea5872",
   "metadata": {},
   "source": [
    "즉, 각 서브레이어(sub-layer)의 출력은 $\\text{LayerNorm}(x + \\text{Sublayer}(x))$  \n",
    "의 형태입니다. 여기서 $\\text{Sublayer}(x)$ 는 해당 서브레이어가 구현하는 함수입니다.  \n",
    "우리는 각 서브레이어의 출력에 **드롭아웃(dropout)**을 적용한 후, 이를 서브레이어의 입력에  \n",
    "더하고 정규화를 수행합니다 ([Dropout](https://jmlr.org/papers/v15/srivastava14a.html)).\n",
    "\n",
    "이러한 **잔차 연결(residual connections)** 을 원활하게 하기 위해, 모델 내의   \n",
    "모든 서브레이어들과 임베딩 레이어는 출력 차원을 $d_{model} = 512$로 맞춥니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cda91-5114-4468-80b5-b7582d7b7cd1",
   "metadata": {},
   "source": [
    "EncoderLayer에서 $size=d_{model}=512$를 의미함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07184705-ad3b-4991-8a94-c3396912204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28766bee-8dce-4c0c-85e4-99f988ef41ad",
   "metadata": {},
   "source": [
    "각 레이어는 두 개의 서브레이어로 구성되어 있습니다.  \n",
    "첫 번째는 멀티-헤드 자기-어텐션(Multi-head Self-Attention) 메커니즘이며,  \n",
    "두 번째는 위치별로 독립적으로 작동하는 완전 연결 피드포워드 네트워크  \n",
    "(Position-wise Fully Connected Feed-Forward Network)입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b101133-e539-4817-8167-e0652a49e1e0",
   "metadata": {},
   "source": [
    "``` python\n",
    "x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "return self.sublayer[1](x, self.feed_forward)\n",
    "```\n",
    "위 코드를 그림으로 표현하면  \n",
    "![](../images/add_norm_connections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9177d33-5189-4821-a47b-25d78e67af37",
   "metadata": {},
   "source": [
    "The encoder is composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33404aab-aa63-4805-b706-615705193adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd81a35-41ba-47d6-b1bb-547dbc0405ac",
   "metadata": {},
   "source": [
    "encoder 출력 마지막에 LayerNorm을 수행하는 것은 논문에 언급되지 않았지만,  \n",
    "논문 저자들의 원래 구현(예: [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor)) 및 후속 PyTorch 구현체들에서는  \n",
    "encoder 마지막에 추가적인 LayerNorm을 수행하는 것이 관행이 되었다고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a30b40-913e-4ebb-911e-56506482e6d0",
   "metadata": {},
   "source": [
    "### hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6823deb6-cdd8-4798-81d5-6e7905a55b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6\n",
    "d_model=512\n",
    "d_ff=2048\n",
    "h=8\n",
    "dropout=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc2634-e0fd-411d-939d-bd20b68c24a5",
   "metadata": {},
   "source": [
    "하이퍼 파라미터를 위의 경우처럼 설정해주면,  \n",
    "각 layer의 입력 출력 데이터의 차원은 (n_batch, seq_len, d_model) 차원이 됨.  \n",
    "여기서 seq_len는 입력 데이터의 문장 길이임.  \n",
    "문장이 \"nice to meet you\" 면, seg_len은 5가 되면,  \n",
    "하지만 학습을 위해 문장의 길이는 통일하는 것으로 보임. 그래서 pad가 필요하고,  \n",
    "mask가 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f6ba63-36c0-4109-b76b-443b787351e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embeddings(d_model,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75901515-c59d-4826-8cd0-dc6bd65e744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "position = PositionalEncoding(d_model, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3712afc5-d89d-40ca-abb3-9656202b3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = EncoderLayer(d_model, c(attn), c(ff), dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5aae637-7ce1-4283-8d47-7a76f0ec77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(encoder_layer, N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb2f390-64f3-43be-9aa2-a482bb8fa757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx input shape :  torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randint(0, 11, size=(1, 10)))\n",
    "xx = embedding(x)\n",
    "print(\"xx input shape : \", xx.shape)\n",
    "pad_mask = create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f31e9807-ea3c-41cf-b924-b875eb25cad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 7, 0, 2, 7, 7, 2, 1, 7, 4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eeafb96-4bb7-421c-bf4b-671f146a2481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = encoder(xx, pad_mask)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
