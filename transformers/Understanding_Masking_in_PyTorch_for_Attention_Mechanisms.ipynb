{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38dcc0f-c431-42c6-ba74-74b7efc2ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1e213-1695-458a-8d23-83a85454fed3",
   "metadata": {},
   "source": [
    "[Understanding Masking in PyTorch for Attention Mechanisms](https://medium.com/@swarms/understanding-masking-in-pytorch-for-attention-mechanisms-e725059fd49f) 를 참고함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad92b3-d266-4b29-adff-0d9266bbca69",
   "metadata": {},
   "source": [
    "## Padding Mask\n",
    "Padding masks are used to ignore the padding tokens in the input sequences.   \n",
    "In NLP tasks, sequences are often padded to the same length to enable batch processing.   Padding tokens should not influence the model’s learning, so we apply a padding mask to   ensure they are ignored.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d2ffb29-8855-4aa8-9f3d-481a93d8c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_padding_mask ( seq, pad_token= 0 ): \n",
    "    mask = (seq == pad_token).unsqueeze( 1 ).unsqueeze( 2 ) \n",
    "    return mask   # (배치 크기, 1, 1, seq_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00adec5b-8d23-4489-a505-a22e2e9bc0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[False, False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "seq = torch.tensor([[7, 6, 0, 0], [1, 2, 3, 0]])\n",
    "padding_mask = create_padding_mask(seq)\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68038224-9563-433f-9140-92f109e2b093",
   "metadata": {},
   "source": [
    "## Sequence Mask\n",
    "Sequence masks are more flexible and can be used to hide arbitrary parts of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96894c-9805-450b-aeb6-c19a851c33a9",
   "metadata": {},
   "source": [
    "torch.triu는 PyTorch에서 **행렬의 상삼각 행렬(upper triangular matrix)** 을\n",
    "추출하거나 생성하는 데 사용되는 함수입니다. \n",
    "이 함수는 행렬의 대각선과 그 위쪽 요소를 유지하고,\n",
    "나머지 요소를 0으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f434a32-bc79-4c1d-8be5-e82745f6dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_mask(seq):\n",
    "    seg_len = seq.size(1)\n",
    "    mask = torch.triu(torch.ones((seq_len,seq_len)), diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9e48a8-e79b-4fb0-8a71-15633f57d5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "seq_len = 4\n",
    "sequence_mask = create_sequence_mask(torch.zeros(seq_len, seq_len))\n",
    "print(sequence_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca6c5d-5724-431e-8260-d7d5a0105bd7",
   "metadata": {},
   "source": [
    "## Look-ahead Mask\n",
    "Look-ahead masks prevent the model from looking at future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a58c54-83f1-427d-a7ab-8dc8862b9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90ab6c1-56f7-4d23-b9d7-3409b9b64b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "look_ahead_mask = create_look_ahead_mask(4)\n",
    "print(look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a1df5d-197a-41dc-8695-1929e428d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "    dk = q.size()[-1]\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9f5b4f9-97c9-4a40-ae3e-dc26b2562890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "d_model = 512\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "\n",
    "q = torch.rand((batch_size, seq_len, d_model))\n",
    "k = torch.rand((batch_size, seq_len, d_model))\n",
    "v = torch.rand((batch_size, seq_len, d_model))\n",
    "mask = create_look_ahead_mask(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14aaf0f9-9554-4bfb-9693-18b393ca4164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]), torch.Size([2, 4, 512]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape, q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03c97697-3400-47e5-babd-61ca3f483473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733e1171-bb99-4822-a02c-055f3070d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "attention_output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13f45e94-7db5-4025-a9da-129e20e9e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_qk = torch.matmul(q, k.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f900182-98e1-43ee-80fc-cb3699fefd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_qk.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
